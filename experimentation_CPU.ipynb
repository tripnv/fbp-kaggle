{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "# Import only in CPU mode\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders.ordinal import OrdinalEncoder\n",
    "from category_encoders.binary import BinaryEncoder\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, matthews_corrcoef, precision_score, recall_score, classification_report\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlflow.autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"train.csv\"\n",
    "test_path =  \"test.csv\"\n",
    "sample_path = \"sample_submission.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(train_path)\n",
    "# test = pd.read_csv(test_path)\n",
    "# sample = pd.read_csv(sample_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['essay_text'] = data['essay_id'].apply(lambda x: open(f\"train/{x}.txt\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_labels_mapping = {\"Ineffective\": 0, \"Adequate\":1, \"Effective\":2}\n",
    "data.discourse_effectiveness = data.discourse_effectiveness.map(target_labels_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    20977\n",
       "2     9326\n",
       "0     6462\n",
       "Name: discourse_effectiveness, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.discourse_effectiveness.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = data.drop('discourse_effectiveness', axis=1, inplace = False), data.discourse_effectiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = .20, random_state=RANDOM_SEED, stratify=data.discourse_effectiveness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29412, 5) (29412,) (7353, 5) (7353,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>essay_id</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>essay_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8629</th>\n",
       "      <td>7d7fb0ac2edb</td>\n",
       "      <td>9C480C68AA9B</td>\n",
       "      <td>Instead of laying on the couch, eating, sleepi...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>If the Summer is plagued with more work we sho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10274</th>\n",
       "      <td>96517470c123</td>\n",
       "      <td>B8B5B46DA523</td>\n",
       "      <td>like some simplified electronics made of silic...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>No one has ever landed on venus so the author ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4293</th>\n",
       "      <td>bb394ddc5bb1</td>\n",
       "      <td>4C51280DE2A8</td>\n",
       "      <td>Second, now to the conspiracy theorists, they ...</td>\n",
       "      <td>Counterclaim</td>\n",
       "      <td>First of all, NASA only gets their information...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2443</th>\n",
       "      <td>d85fb9fc13c9</td>\n",
       "      <td>2D08A68E70CD</td>\n",
       "      <td>Third example has pathos catching peoples feel...</td>\n",
       "      <td>Claim</td>\n",
       "      <td>I think that the author describes how technolg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16589</th>\n",
       "      <td>2f5adb92fe7d</td>\n",
       "      <td>1EFA2916E5A8</td>\n",
       "      <td>it would also in the world of to day make him ...</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Dear Principle,\\n\\nI personally do not think s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3015</th>\n",
       "      <td>6fb01a8a829a</td>\n",
       "      <td>37FC9DB2D1DB</td>\n",
       "      <td>it's their summer.</td>\n",
       "      <td>Claim</td>\n",
       "      <td>When assigned a project during summer break, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26587</th>\n",
       "      <td>7551a7b008f5</td>\n",
       "      <td>A4C9096A123B</td>\n",
       "      <td>I think people should be able to choose who t...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Dear, State Sentor\\n\\nI think the electoral co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19477</th>\n",
       "      <td>0f5d0b88c638</td>\n",
       "      <td>44DEA88FDD83</td>\n",
       "      <td>But you see there is up side to using the Elec...</td>\n",
       "      <td>Counterclaim</td>\n",
       "      <td>Dear Floridas state senator, I am righting thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30083</th>\n",
       "      <td>2d7b19e2991b</td>\n",
       "      <td>D786FC589E93</td>\n",
       "      <td>but we should at least get a vote on like new ...</td>\n",
       "      <td>Rebuttal</td>\n",
       "      <td>Dear senator,\\n\\nGetting ride of the Electoral...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7207</th>\n",
       "      <td>a08955d08d6c</td>\n",
       "      <td>82514A286403</td>\n",
       "      <td>When they unveiled the image for all to see th...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>The face is a landform because the planet must...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29412 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       discourse_id      essay_id  \\\n",
       "8629   7d7fb0ac2edb  9C480C68AA9B   \n",
       "10274  96517470c123  B8B5B46DA523   \n",
       "4293   bb394ddc5bb1  4C51280DE2A8   \n",
       "2443   d85fb9fc13c9  2D08A68E70CD   \n",
       "16589  2f5adb92fe7d  1EFA2916E5A8   \n",
       "...             ...           ...   \n",
       "3015   6fb01a8a829a  37FC9DB2D1DB   \n",
       "26587  7551a7b008f5  A4C9096A123B   \n",
       "19477  0f5d0b88c638  44DEA88FDD83   \n",
       "30083  2d7b19e2991b  D786FC589E93   \n",
       "7207   a08955d08d6c  82514A286403   \n",
       "\n",
       "                                          discourse_text discourse_type  \\\n",
       "8629   Instead of laying on the couch, eating, sleepi...       Evidence   \n",
       "10274  like some simplified electronics made of silic...       Evidence   \n",
       "4293   Second, now to the conspiracy theorists, they ...   Counterclaim   \n",
       "2443   Third example has pathos catching peoples feel...          Claim   \n",
       "16589  it would also in the world of to day make him ...          Claim   \n",
       "...                                                  ...            ...   \n",
       "3015                                 it's their summer.           Claim   \n",
       "26587   I think people should be able to choose who t...       Evidence   \n",
       "19477  But you see there is up side to using the Elec...   Counterclaim   \n",
       "30083  but we should at least get a vote on like new ...       Rebuttal   \n",
       "7207   When they unveiled the image for all to see th...       Evidence   \n",
       "\n",
       "                                              essay_text  \n",
       "8629   If the Summer is plagued with more work we sho...  \n",
       "10274  No one has ever landed on venus so the author ...  \n",
       "4293   First of all, NASA only gets their information...  \n",
       "2443   I think that the author describes how technolg...  \n",
       "16589  Dear Principle,\\n\\nI personally do not think s...  \n",
       "...                                                  ...  \n",
       "3015   When assigned a project during summer break, d...  \n",
       "26587  Dear, State Sentor\\n\\nI think the electoral co...  \n",
       "19477  Dear Floridas state senator, I am righting thi...  \n",
       "30083  Dear senator,\\n\\nGetting ride of the Electoral...  \n",
       "7207   The face is a landform because the planet must...  \n",
       "\n",
       "[29412 rows x 5 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "996"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LogReg\n",
    "\n",
    "No scaling\n",
    "Min loss \t0.7707357995019517 <br>\n",
    "Max loss \t0.7939273816408711 <br>\n",
    "Mean loss\t0.7830213020862644 <br>\n",
    "\n",
    "\n",
    "\n",
    "Scaled w/ MaxAbsScaler\n",
    "Min loss \t0.8006261104803448 <br>\n",
    "Max loss \t0.8303430962728789 <br>\n",
    "Mean loss\t0.8157244737733116 <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/08/10 19:33:56 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '638d321d45b74353a961073a0dfcafa9', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/experimental/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "2022/08/10 19:33:58 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/opt/homebrew/Caskroom/miniforge/base/envs/experimental/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator.\"\n",
      "2022/08/10 19:33:58 WARNING mlflow.sklearn: Failed to infer model signature: Expected one of (pandas.DataFrame, numpy array, dictionary of (name -> numpy.ndarray), pyspark.sql.DataFrame) but got '<class 'scipy.sparse._csr.csr_matrix'>'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.7869757143697128\n",
      "................................................................................\n",
      "\n",
      "Fold # 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/08/10 19:34:01 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'ae5b9e6c135048d5aa33a40fa5259b82', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/experimental/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "2022/08/10 19:34:02 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/opt/homebrew/Caskroom/miniforge/base/envs/experimental/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator.\"\n",
      "2022/08/10 19:34:02 WARNING mlflow.sklearn: Failed to infer model signature: Expected one of (pandas.DataFrame, numpy array, dictionary of (name -> numpy.ndarray), pyspark.sql.DataFrame) but got '<class 'scipy.sparse._csr.csr_matrix'>'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.7804193422274297\n",
      "................................................................................\n",
      "\n",
      "Fold # 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/08/10 19:34:04 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '6e4687e4886b4058878ca20c93e3abff', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/experimental/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "2022/08/10 19:34:06 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/opt/homebrew/Caskroom/miniforge/base/envs/experimental/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator.\"\n",
      "2022/08/10 19:34:06 WARNING mlflow.sklearn: Failed to infer model signature: Expected one of (pandas.DataFrame, numpy array, dictionary of (name -> numpy.ndarray), pyspark.sql.DataFrame) but got '<class 'scipy.sparse._csr.csr_matrix'>'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.7939273816408711\n",
      "................................................................................\n",
      "\n",
      "Fold # 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/08/10 19:34:08 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'aa7c12f16ef245a3b53bf7a93ae34c35', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/experimental/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "2022/08/10 19:34:09 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/opt/homebrew/Caskroom/miniforge/base/envs/experimental/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator.\"\n",
      "2022/08/10 19:34:09 WARNING mlflow.sklearn: Failed to infer model signature: Expected one of (pandas.DataFrame, numpy array, dictionary of (name -> numpy.ndarray), pyspark.sql.DataFrame) but got '<class 'scipy.sparse._csr.csr_matrix'>'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.7830482726913571\n",
      "................................................................................\n",
      "\n",
      "Fold # 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/08/10 19:34:12 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '57204076de6b4808bb46ba8d7c5a7bea', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/experimental/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "2022/08/10 19:34:13 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/opt/homebrew/Caskroom/miniforge/base/envs/experimental/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator.\"\n",
      "2022/08/10 19:34:13 WARNING mlflow.sklearn: Failed to infer model signature: Expected one of (pandas.DataFrame, numpy array, dictionary of (name -> numpy.ndarray), pyspark.sql.DataFrame) but got '<class 'scipy.sparse._csr.csr_matrix'>'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.7707357995019517\n",
      "................................................................................\n",
      "--------------------------------------------------------------------------------\n",
      "Min loss \t0.7707357995019517\n",
      "Max loss \t0.7939273816408711\n",
      "Mean loss\t0.7830213020862644\n"
     ]
    }
   ],
   "source": [
    "stratified_k = StratifiedKFold(n_splits = 5, shuffle=True, random_state = RANDOM_SEED)\n",
    "losses = []\n",
    "for idx, (train_idxs, valid_idxs) in enumerate(stratified_k.split(X_train, Y_train)):\n",
    "    print(f\"\\nFold # {idx + 1}\")\n",
    "\n",
    "    x_train, y_train = X_train.iloc[train_idxs], Y_train.iloc[train_idxs]\n",
    "    x_valid, y_valid = X_train.iloc[valid_idxs], Y_train.iloc[valid_idxs]\n",
    "\n",
    "    y_train = y_train.to_numpy()\n",
    "    y_valid = y_valid.to_numpy()\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(X_train.essay_text.drop_duplicates())\n",
    "\n",
    "    vectorized_discourse_text_train = vectorizer.transform(x_train.discourse_text)\n",
    "    vectorized_discourse_text_test = vectorizer.transform(x_valid.discourse_text)\n",
    "\n",
    "    ohe = OneHotEncoder()\n",
    "    ohe_discourse_type_train = ohe.fit_transform(x_train.discourse_type.values.reshape(-1, 1))\n",
    "    ohe_discourse_type_test = ohe.transform(x_valid.discourse_type.values.reshape(-1, 1))\n",
    "\n",
    "    x_train = sparse.hstack([vectorized_discourse_text_train, ohe_discourse_type_train])\n",
    "    x_valid = sparse.hstack([vectorized_discourse_text_test, ohe_discourse_type_test])\n",
    "\n",
    "    # In case it is necessary\n",
    "    # scaler = MaxAbsScaler()\n",
    "    # x_train = scaler.fit_transform(x_train)\n",
    "    # x_valid = scaler.transform(x_valid)\n",
    "\n",
    "    model = LogisticRegression()\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    prob_predictions = model.predict_proba(x_valid)\n",
    "    lloss = log_loss(y_valid, prob_predictions)\n",
    "    losses.append(lloss)\n",
    "\n",
    "    print(f\"\\tLog loss: {lloss}\")\n",
    "    print(80*'.')\n",
    "\n",
    "print(80*'-')\n",
    "\n",
    "print(f\"Min loss \\t{min(losses)}\")\n",
    "print(f\"Max loss \\t{max(losses)}\")\n",
    "print(f\"Mean loss\\t{np.mean(losses)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RF\n",
    "\n",
    "Min loss \t0.8211464183621852 <br>\n",
    "Max loss \t0.8326483703178386 <br>\n",
    "Mean loss\t0.8274193801826459 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold # 1\n",
      "\tLog loss: 0.8179767168529476\n",
      "................................................................................\n",
      "\n",
      "Fold # 2\n"
     ]
    }
   ],
   "source": [
    "stratified_k = StratifiedKFold(n_splits = 5, shuffle=True, random_state = RANDOM_SEED)\n",
    "losses = []\n",
    "for idx, (train_idxs, valid_idxs) in enumerate(stratified_k.split(X_train, Y_train)):\n",
    "    print(f\"\\nFold # {idx + 1}\")\n",
    "\n",
    "    x_train, y_train = X_train.iloc[train_idxs], Y_train.iloc[train_idxs]\n",
    "    x_valid, y_valid = X_train.iloc[valid_idxs], Y_train.iloc[valid_idxs]\n",
    "\n",
    "    y_train = y_train.to_numpy()\n",
    "    y_valid = y_valid.to_numpy()\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(X_train.essay_text.drop_duplicates())\n",
    "\n",
    "    vectorized_discourse_text_train = vectorizer.transform(x_train.discourse_text)\n",
    "    vectorized_discourse_text_test = vectorizer.transform(x_valid.discourse_text)\n",
    "\n",
    "    ohe = OneHotEncoder()\n",
    "    ohe_discourse_type_train = ohe.fit_transform(x_train.discourse_type.values.reshape(-1, 1))\n",
    "    ohe_discourse_type_test = ohe.transform(x_valid.discourse_type.values.reshape(-1, 1))\n",
    "\n",
    "    x_train = sparse.hstack([vectorized_discourse_text_train, ohe_discourse_type_train])\n",
    "    x_valid = sparse.hstack([vectorized_discourse_text_test, ohe_discourse_type_test])\n",
    "\n",
    "    # In case it is necessary\n",
    "    # scaler = MaxAbsScaler()\n",
    "    # x_train = scaler.fit_transform(x_train)\n",
    "    # x_valid = scaler.transform(x_valid)\n",
    "\n",
    "    model = RandomForestClassifier(n_estimators=1000)\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    prob_predictions = model.predict_proba(x_valid)\n",
    "    lloss = log_loss(y_valid, prob_predictions)\n",
    "    losses.append(lloss)\n",
    "\n",
    "    print(f\"\\tLog loss: {lloss}\")\n",
    "    print(80*'.')\n",
    "\n",
    "print(80*'-')\n",
    "\n",
    "print(f\"Min loss \\t{min(losses)}\")\n",
    "print(f\"Max loss \\t{max(losses)}\")\n",
    "print(f\"Mean loss\\t{np.mean(losses)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost\n",
    "Min loss \t0.769898167914014 <br>\n",
    "Max loss \t0.7861944754332157 <br>\n",
    "Mean loss\t0.7808887629128117 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/08/10 19:26:25 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '85b98ebfe07c4f72811640207eac6e0f', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current xgboost workflow\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/experimental/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:26:26] WARNING: /private/var/folders/c6/5pqpts4j0z714gbd4lgp_1g80000gn/T/pip-install-v5ts534i/xgboost_d6868f90e2e54d9f93ddde7d76574102/build/temp.macosx-11.0-arm64-3.9/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/08/10 19:27:12 WARNING mlflow.sklearn: Failed to infer model signature: Expected one of (pandas.DataFrame, numpy array, dictionary of (name -> numpy.ndarray), pyspark.sql.DataFrame) but got '<class 'scipy.sparse._csr.csr_matrix'>'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.7831609343908684\n",
      "................................................................................\n",
      "\n",
      "Fold # 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/08/10 19:27:15 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '2d0aa38649744f759bd89ba849c8939c', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current xgboost workflow\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/experimental/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:27:15] WARNING: /private/var/folders/c6/5pqpts4j0z714gbd4lgp_1g80000gn/T/pip-install-v5ts534i/xgboost_d6868f90e2e54d9f93ddde7d76574102/build/temp.macosx-11.0-arm64-3.9/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/08/10 19:28:02 WARNING mlflow.sklearn: Failed to infer model signature: Expected one of (pandas.DataFrame, numpy array, dictionary of (name -> numpy.ndarray), pyspark.sql.DataFrame) but got '<class 'scipy.sparse._csr.csr_matrix'>'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.7817585589293724\n",
      "................................................................................\n",
      "\n",
      "Fold # 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/08/10 19:28:04 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '17153cd8e63a4ec899b8ea9934ed18bd', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current xgboost workflow\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/experimental/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:28:05] WARNING: /private/var/folders/c6/5pqpts4j0z714gbd4lgp_1g80000gn/T/pip-install-v5ts534i/xgboost_d6868f90e2e54d9f93ddde7d76574102/build/temp.macosx-11.0-arm64-3.9/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/08/10 19:28:51 WARNING mlflow.sklearn: Failed to infer model signature: Expected one of (pandas.DataFrame, numpy array, dictionary of (name -> numpy.ndarray), pyspark.sql.DataFrame) but got '<class 'scipy.sparse._csr.csr_matrix'>'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.7834316778965883\n",
      "................................................................................\n",
      "\n",
      "Fold # 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/08/10 19:28:54 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'f2809c6dbda04f99977e906af55cc29d', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current xgboost workflow\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/experimental/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:28:54] WARNING: /private/var/folders/c6/5pqpts4j0z714gbd4lgp_1g80000gn/T/pip-install-v5ts534i/xgboost_d6868f90e2e54d9f93ddde7d76574102/build/temp.macosx-11.0-arm64-3.9/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/08/10 19:29:41 WARNING mlflow.sklearn: Failed to infer model signature: Expected one of (pandas.DataFrame, numpy array, dictionary of (name -> numpy.ndarray), pyspark.sql.DataFrame) but got '<class 'scipy.sparse._csr.csr_matrix'>'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.7861944754332157\n",
      "................................................................................\n",
      "\n",
      "Fold # 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/08/10 19:29:43 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '9eecbea3e7c749559c4836195ed9217c', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current xgboost workflow\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/experimental/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:29:44] WARNING: /private/var/folders/c6/5pqpts4j0z714gbd4lgp_1g80000gn/T/pip-install-v5ts534i/xgboost_d6868f90e2e54d9f93ddde7d76574102/build/temp.macosx-11.0-arm64-3.9/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/08/10 19:30:31 WARNING mlflow.sklearn: Failed to infer model signature: Expected one of (pandas.DataFrame, numpy array, dictionary of (name -> numpy.ndarray), pyspark.sql.DataFrame) but got '<class 'scipy.sparse._csr.csr_matrix'>'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.769898167914014\n",
      "................................................................................\n",
      "--------------------------------------------------------------------------------\n",
      "Min loss \t0.769898167914014\n",
      "Max loss \t0.7861944754332157\n",
      "Mean loss\t0.7808887629128117\n"
     ]
    }
   ],
   "source": [
    "stratified_k = StratifiedKFold(n_splits = 5, shuffle=True, random_state = RANDOM_SEED)\n",
    "losses = []\n",
    "for idx, (train_idxs, valid_idxs) in enumerate(stratified_k.split(X_train, Y_train)):\n",
    "    print(f\"\\nFold # {idx + 1}\")\n",
    "\n",
    "    x_train, y_train = X_train.iloc[train_idxs], Y_train.iloc[train_idxs]\n",
    "    x_valid, y_valid = X_train.iloc[valid_idxs], Y_train.iloc[valid_idxs]\n",
    "\n",
    "    y_train = y_train.to_numpy()\n",
    "    y_valid = y_valid.to_numpy()\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(X_train.essay_text.drop_duplicates())\n",
    "\n",
    "    vectorized_discourse_text_train = vectorizer.transform(x_train.discourse_text)\n",
    "    vectorized_discourse_text_test = vectorizer.transform(x_valid.discourse_text)\n",
    "\n",
    "    ohe = OneHotEncoder()\n",
    "    ohe_discourse_type_train = ohe.fit_transform(x_train.discourse_type.values.reshape(-1, 1))\n",
    "    ohe_discourse_type_test = ohe.transform(x_valid.discourse_type.values.reshape(-1, 1))\n",
    "\n",
    "    x_train = sparse.hstack([vectorized_discourse_text_train, ohe_discourse_type_train])\n",
    "    x_valid = sparse.hstack([vectorized_discourse_text_test, ohe_discourse_type_test])\n",
    "\n",
    "    # In case it is necessary\n",
    "    # scaler = MaxAbsScaler()\n",
    "    # x_train = scaler.fit_transform(x_train)\n",
    "    # x_valid = scaler.transform(x_valid)\n",
    "\n",
    "    model = XGBClassifier()\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    prob_predictions = model.predict_proba(x_valid)\n",
    "    lloss = log_loss(y_valid, prob_predictions)\n",
    "    losses.append(lloss)\n",
    "\n",
    "    print(f\"\\tLog loss: {lloss}\")\n",
    "    print(80*'.')\n",
    "\n",
    "print(80*'-')\n",
    "\n",
    "print(f\"Min loss \\t{min(losses)}\")\n",
    "print(f\"Max loss \\t{max(losses)}\")\n",
    "print(f\"Mean loss\\t{np.mean(losses)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LGBM\n",
    "Min loss \t0.7565350176859863 <br>\n",
    "Max loss \t0.7765817628486158 <br>\n",
    "Mean loss\t0.7692048809033425 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold # 1\n",
      "\tLog loss: 0.7698034071294326\n",
      "................................................................................\n",
      "\n",
      "Fold # 2\n",
      "\tLog loss: 0.7695595489450545\n",
      "................................................................................\n",
      "\n",
      "Fold # 3\n",
      "\tLog loss: 0.7765817628486158\n",
      "................................................................................\n",
      "\n",
      "Fold # 4\n",
      "\tLog loss: 0.773544667907623\n",
      "................................................................................\n",
      "\n",
      "Fold # 5\n",
      "\tLog loss: 0.7565350176859863\n",
      "................................................................................\n",
      "--------------------------------------------------------------------------------\n",
      "Min loss \t0.7565350176859863\n",
      "Max loss \t0.7765817628486158\n",
      "Mean loss\t0.7692048809033425\n"
     ]
    }
   ],
   "source": [
    "stratified_k = StratifiedKFold(n_splits = 5, shuffle=True, random_state = RANDOM_SEED)\n",
    "losses = []\n",
    "for idx, (train_idxs, valid_idxs) in enumerate(stratified_k.split(X_train, Y_train)):\n",
    "    print(f\"\\nFold # {idx + 1}\")\n",
    "\n",
    "    x_train, y_train = X_train.iloc[train_idxs], Y_train.iloc[train_idxs]\n",
    "    x_valid, y_valid = X_train.iloc[valid_idxs], Y_train.iloc[valid_idxs]\n",
    "\n",
    "    y_train = y_train.to_numpy()\n",
    "    y_valid = y_valid.to_numpy()\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(X_train.essay_text.drop_duplicates())\n",
    "\n",
    "    vectorized_discourse_text_train = vectorizer.transform(x_train.discourse_text)\n",
    "    vectorized_discourse_text_test = vectorizer.transform(x_valid.discourse_text)\n",
    "\n",
    "    ohe = OneHotEncoder()\n",
    "    ohe_discourse_type_train = ohe.fit_transform(x_train.discourse_type.values.reshape(-1, 1))\n",
    "    ohe_discourse_type_test = ohe.transform(x_valid.discourse_type.values.reshape(-1, 1))\n",
    "\n",
    "    x_train = sparse.hstack([vectorized_discourse_text_train, ohe_discourse_type_train])\n",
    "    x_valid = sparse.hstack([vectorized_discourse_text_test, ohe_discourse_type_test])\n",
    "\n",
    "    # In case it is necessary\n",
    "    # scaler = MaxAbsScaler()\n",
    "    # x_train = scaler.fit_transform(x_train)\n",
    "    # x_valid = scaler.transform(x_valid)\n",
    "\n",
    "    model = LGBMClassifier()\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    prob_predictions = model.predict_proba(x_valid)\n",
    "    lloss = log_loss(y_valid, prob_predictions)\n",
    "    losses.append(lloss)\n",
    "\n",
    "    print(f\"\\tLog loss: {lloss}\")\n",
    "    print(80*'.')\n",
    "\n",
    "print(80*'-')\n",
    "\n",
    "print(f\"Min loss \\t{min(losses)}\")\n",
    "print(f\"Max loss \\t{max(losses)}\")\n",
    "print(f\"Mean loss\\t{np.mean(losses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7353, 3)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7759086348638186"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = log_loss(y_true = Y_test, y_pred = Y_pred)\n",
    "loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HistGradientBoosting\n",
    "Min loss \t0.7589378423671769 <br>\n",
    "Max loss \t0.7814936649694296 <br>\n",
    "Mean loss\t0.7706771662759752 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.7725146625309408\n",
      "................................................................................\n",
      "\n",
      "Fold # 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.7672879132724585\n",
      "................................................................................\n",
      "\n",
      "Fold # 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.7803359314651005\n",
      "................................................................................\n",
      "\n",
      "Fold # 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.7760577902829843\n",
      "................................................................................\n",
      "\n",
      "Fold # 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.7572928859563774\n",
      "................................................................................\n",
      "--------------------------------------------------------------------------------\n",
      "Min loss \t0.7572928859563774\n",
      "Max loss \t0.7803359314651005\n",
      "Mean loss\t0.7706978367015722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "stratified_k = StratifiedKFold(n_splits = 5, shuffle=True, random_state = RANDOM_SEED)\n",
    "losses = []\n",
    "for idx, (train_idxs, valid_idxs) in enumerate(stratified_k.split(X_train, Y_train)):\n",
    "    print(f\"\\nFold # {idx + 1}\")\n",
    "\n",
    "    x_train, y_train = X_train.iloc[train_idxs], Y_train.iloc[train_idxs]\n",
    "    x_valid, y_valid = X_train.iloc[valid_idxs], Y_train.iloc[valid_idxs]\n",
    "\n",
    "    y_train = y_train.to_numpy()\n",
    "    y_valid = y_valid.to_numpy()\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(X_train.essay_text.drop_duplicates())\n",
    "\n",
    "    vectorized_discourse_text_train = vectorizer.transform(x_train.discourse_text)\n",
    "    vectorized_discourse_text_test = vectorizer.transform(x_valid.discourse_text)\n",
    "\n",
    "    ohe = OneHotEncoder()\n",
    "    ohe_discourse_type_train = ohe.fit_transform(x_train.discourse_type.values.reshape(-1, 1))\n",
    "    ohe_discourse_type_test = ohe.transform(x_valid.discourse_type.values.reshape(-1, 1))\n",
    "\n",
    "    x_train = sparse.hstack([vectorized_discourse_text_train, ohe_discourse_type_train])\n",
    "    x_valid = sparse.hstack([vectorized_discourse_text_test, ohe_discourse_type_test])\n",
    "\n",
    "    # In case it is necessary\n",
    "    # scaler = MaxAbsScaler()\n",
    "    # x_train = scaler.fit_transform(x_train)\n",
    "    # x_valid = scaler.transform(x_valid)\n",
    "\n",
    "    x_train, x_valid = x_train.todense(), x_valid.todense()\n",
    "\n",
    "    model = HistGradientBoostingClassifier()\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    prob_predictions = model.predict_proba(x_valid)\n",
    "    lloss = log_loss(y_valid, prob_predictions)\n",
    "    losses.append(lloss)\n",
    "\n",
    "    print(f\"\\tLog loss: {lloss}\")\n",
    "    print(80*'.')\n",
    "\n",
    "print(80*'-')\n",
    "\n",
    "print(f\"Min loss \\t{min(losses)}\")\n",
    "print(f\"Max loss \\t{max(losses)}\")\n",
    "print(f\"Mean loss\\t{np.mean(losses)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, CategoricalNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 1.0052641991904334\n",
      "................................................................................\n",
      "\n",
      "Fold # 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.9989523898046153\n",
      "................................................................................\n",
      "\n",
      "Fold # 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 1.0012554263271314\n",
      "................................................................................\n",
      "\n",
      "Fold # 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 1.00147215303088\n",
      "................................................................................\n",
      "\n",
      "Fold # 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.9826888223621003\n",
      "................................................................................\n",
      "--------------------------------------------------------------------------------\n",
      "Min loss \t0.9826888223621003\n",
      "Max loss \t1.0052641991904334\n",
      "Mean loss\t0.9979265981430322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "stratified_k = StratifiedKFold(n_splits = 5, shuffle=True, random_state = RANDOM_SEED)\n",
    "losses = []\n",
    "for idx, (train_idxs, valid_idxs) in enumerate(stratified_k.split(X_train, Y_train)):\n",
    "    print(f\"\\nFold # {idx + 1}\")\n",
    "\n",
    "    x_train, y_train = X_train.iloc[train_idxs], Y_train.iloc[train_idxs]\n",
    "    x_valid, y_valid = X_train.iloc[valid_idxs], Y_train.iloc[valid_idxs]\n",
    "\n",
    "    y_train = y_train.to_numpy()\n",
    "    y_valid = y_valid.to_numpy()\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(X_train.essay_text.drop_duplicates())\n",
    "\n",
    "    vectorized_discourse_text_train = vectorizer.transform(x_train.discourse_text)\n",
    "    vectorized_discourse_text_test = vectorizer.transform(x_valid.discourse_text)\n",
    "\n",
    "    ohe = OneHotEncoder()\n",
    "    ohe_discourse_type_train = ohe.fit_transform(x_train.discourse_type.values.reshape(-1, 1))\n",
    "    ohe_discourse_type_test = ohe.transform(x_valid.discourse_type.values.reshape(-1, 1))\n",
    "\n",
    "    x_train = sparse.hstack([vectorized_discourse_text_train, ohe_discourse_type_train])\n",
    "    x_valid = sparse.hstack([vectorized_discourse_text_test, ohe_discourse_type_test])\n",
    "\n",
    "    # In case it is necessary\n",
    "    # scaler = MaxAbsScaler()\n",
    "    # x_train = scaler.fit_transform(x_train)\n",
    "    # x_valid = scaler.transform(x_valid)\n",
    "\n",
    "    x_train, x_valid = x_train.todense(), x_valid.todense()\n",
    "\n",
    "    model = MultinomialNB()\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    prob_predictions = model.predict_proba(x_valid)\n",
    "    lloss = log_loss(y_valid, prob_predictions)\n",
    "    losses.append(lloss)\n",
    "\n",
    "    print(f\"\\tLog loss: {lloss}\")\n",
    "    print(80*'.')\n",
    "\n",
    "print(80*'-')\n",
    "\n",
    "print(f\"Min loss \\t{min(losses)}\")\n",
    "print(f\"Max loss \\t{max(losses)}\")\n",
    "print(f\"Mean loss\\t{np.mean(losses)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "w/ more data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.9541224343739063\n",
      "................................................................................\n",
      "--------------------------------------------------------------------------------\n",
      "Min loss \t0.9541224343739063\n",
      "Max loss \t0.9541224343739063\n",
      "Mean loss\t0.9541224343739063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "x_train, y_train = X_train, Y_train\n",
    "x_valid, y_valid = X_test, Y_test\n",
    "\n",
    "y_train = y_train.to_numpy()\n",
    "y_valid = y_valid.to_numpy()\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(X_train.essay_text.drop_duplicates())\n",
    "\n",
    "vectorized_discourse_text_train = vectorizer.transform(x_train.discourse_text)\n",
    "vectorized_discourse_text_test = vectorizer.transform(x_valid.discourse_text)\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "ohe_discourse_type_train = ohe.fit_transform(x_train.discourse_type.values.reshape(-1, 1))\n",
    "ohe_discourse_type_test = ohe.transform(x_valid.discourse_type.values.reshape(-1, 1))\n",
    "\n",
    "x_train = sparse.hstack([vectorized_discourse_text_train, ohe_discourse_type_train])\n",
    "x_valid = sparse.hstack([vectorized_discourse_text_test, ohe_discourse_type_test])\n",
    "\n",
    "# In case it is necessary\n",
    "# scaler = MaxAbsScaler()\n",
    "# x_train = scaler.fit_transform(x_train)\n",
    "# x_valid = scaler.transform(x_valid)\n",
    "\n",
    "x_train, x_valid = x_train.todense(), x_valid.todense()\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "prob_predictions = model.predict_proba(x_valid)\n",
    "lloss = log_loss(y_valid, prob_predictions)\n",
    "losses.append(lloss)\n",
    "\n",
    "print(f\"\\tLog loss: {lloss}\")\n",
    "print(80*'.')\n",
    "\n",
    "print(80*'-')\n",
    "\n",
    "print(f\"Min loss \\t{min(losses)}\")\n",
    "print(f\"Max loss \\t{max(losses)}\")\n",
    "print(f\"Mean loss\\t{np.mean(losses)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModel\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading tokenizer_config.json: 100%|██████████| 29.0/29.0 [00:00<00:00, 14.0kB/s]\n",
      "Downloading vocab.txt: 100%|██████████| 208k/208k [00:00<00:00, 434kB/s]  \n",
      "Downloading tokenizer.json: 100%|██████████| 426k/426k [00:00<00:00, 714kB/s] \n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading config.json: 100%|██████████| 570/570 [00:00<00:00, 276kB/s]\n",
      "2022-08-14 14:24:35.931994: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "Downloading tf_model.h5: 100%|██████████| 502M/502M [00:17<00:00, 30.0MB/s] \n",
      "2022-08-14 14:24:55.035998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-14 14:24:55.039555: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-14 14:24:55.039717: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-14 14:24:55.040068: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-08-14 14:24:55.041100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-14 14:24:55.041265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-14 14:24:55.041581: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-14 14:24:55.496612: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-14 14:24:55.496737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-14 14:24:55.496803: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-14 14:24:55.496878: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21596 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2022-08-14 14:24:56.311013: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModel.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_head = TFAutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b7d9b83467eda07c7dcec41582b0623e164b4685431fddd627aa809781990b03"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 ('exp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "758258b5e8b45ec9f5b1e8a0f832f1f98ad6752149e10d5e95f53d93cab8a1dc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
