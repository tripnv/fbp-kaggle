{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import only in CPU mode\n",
    "# from sklearnex import patch_sklearn\n",
    "# patch_sklearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders.ordinal import OrdinalEncoder\n",
    "from category_encoders.binary import BinaryEncoder\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "from sklearn.metrics import log_loss, accuracy_score, matthews_corrcoef\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, matthews_corrcoef, precision_score, recall_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spylls.hunspell import Dictionary\n",
    "from string import punctuation\n",
    "import re\n",
    "import contractions\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyphenate import hyphenate_word\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlflow.autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"train.csv\"\n",
    "test_path =  \"test.csv\"\n",
    "sample_path = \"sample_submission.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(train_path)\n",
    "# test = pd.read_csv(test_path)\n",
    "# sample = pd.read_csv(sample_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['essay_text'] = data['essay_id'].apply(lambda x: open(f\"train/{x}.txt\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_labels_mapping = {\"Ineffective\": 0, \"Adequate\":1, \"Effective\":2}\n",
    "data.discourse_effectiveness = data.discourse_effectiveness.map(target_labels_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    20977\n",
       "2     9326\n",
       "0     6462\n",
       "Name: discourse_effectiveness, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.discourse_effectiveness.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = data.drop('discourse_effectiveness', axis=1, inplace = False), data.discourse_effectiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = .20, random_state=RANDOM_SEED, stratify=data.discourse_effectiveness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29412, 5) (29412,) (7353, 5) (7353,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>essay_id</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>essay_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8629</th>\n",
       "      <td>7d7fb0ac2edb</td>\n",
       "      <td>9C480C68AA9B</td>\n",
       "      <td>Instead of laying on the couch, eating, sleepi...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>If the Summer is plagued with more work we sho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10274</th>\n",
       "      <td>96517470c123</td>\n",
       "      <td>B8B5B46DA523</td>\n",
       "      <td>like some simplified electronics made of silic...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>No one has ever landed on venus so the author ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4293</th>\n",
       "      <td>bb394ddc5bb1</td>\n",
       "      <td>4C51280DE2A8</td>\n",
       "      <td>Second, now to the conspiracy theorists, they ...</td>\n",
       "      <td>Counterclaim</td>\n",
       "      <td>First of all, NASA only gets their information...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2443</th>\n",
       "      <td>d85fb9fc13c9</td>\n",
       "      <td>2D08A68E70CD</td>\n",
       "      <td>Third example has pathos catching peoples feel...</td>\n",
       "      <td>Claim</td>\n",
       "      <td>I think that the author describes how technolg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16589</th>\n",
       "      <td>2f5adb92fe7d</td>\n",
       "      <td>1EFA2916E5A8</td>\n",
       "      <td>it would also in the world of to day make him ...</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Dear Principle,\\n\\nI personally do not think s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3015</th>\n",
       "      <td>6fb01a8a829a</td>\n",
       "      <td>37FC9DB2D1DB</td>\n",
       "      <td>it's their summer.</td>\n",
       "      <td>Claim</td>\n",
       "      <td>When assigned a project during summer break, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26587</th>\n",
       "      <td>7551a7b008f5</td>\n",
       "      <td>A4C9096A123B</td>\n",
       "      <td>I think people should be able to choose who t...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Dear, State Sentor\\n\\nI think the electoral co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19477</th>\n",
       "      <td>0f5d0b88c638</td>\n",
       "      <td>44DEA88FDD83</td>\n",
       "      <td>But you see there is up side to using the Elec...</td>\n",
       "      <td>Counterclaim</td>\n",
       "      <td>Dear Floridas state senator, I am righting thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30083</th>\n",
       "      <td>2d7b19e2991b</td>\n",
       "      <td>D786FC589E93</td>\n",
       "      <td>but we should at least get a vote on like new ...</td>\n",
       "      <td>Rebuttal</td>\n",
       "      <td>Dear senator,\\n\\nGetting ride of the Electoral...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7207</th>\n",
       "      <td>a08955d08d6c</td>\n",
       "      <td>82514A286403</td>\n",
       "      <td>When they unveiled the image for all to see th...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>The face is a landform because the planet must...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29412 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       discourse_id      essay_id  \\\n",
       "8629   7d7fb0ac2edb  9C480C68AA9B   \n",
       "10274  96517470c123  B8B5B46DA523   \n",
       "4293   bb394ddc5bb1  4C51280DE2A8   \n",
       "2443   d85fb9fc13c9  2D08A68E70CD   \n",
       "16589  2f5adb92fe7d  1EFA2916E5A8   \n",
       "...             ...           ...   \n",
       "3015   6fb01a8a829a  37FC9DB2D1DB   \n",
       "26587  7551a7b008f5  A4C9096A123B   \n",
       "19477  0f5d0b88c638  44DEA88FDD83   \n",
       "30083  2d7b19e2991b  D786FC589E93   \n",
       "7207   a08955d08d6c  82514A286403   \n",
       "\n",
       "                                          discourse_text discourse_type  \\\n",
       "8629   Instead of laying on the couch, eating, sleepi...       Evidence   \n",
       "10274  like some simplified electronics made of silic...       Evidence   \n",
       "4293   Second, now to the conspiracy theorists, they ...   Counterclaim   \n",
       "2443   Third example has pathos catching peoples feel...          Claim   \n",
       "16589  it would also in the world of to day make him ...          Claim   \n",
       "...                                                  ...            ...   \n",
       "3015                                 it's their summer.           Claim   \n",
       "26587   I think people should be able to choose who t...       Evidence   \n",
       "19477  But you see there is up side to using the Elec...   Counterclaim   \n",
       "30083  but we should at least get a vote on like new ...       Rebuttal   \n",
       "7207   When they unveiled the image for all to see th...       Evidence   \n",
       "\n",
       "                                              essay_text  \n",
       "8629   If the Summer is plagued with more work we sho...  \n",
       "10274  No one has ever landed on venus so the author ...  \n",
       "4293   First of all, NASA only gets their information...  \n",
       "2443   I think that the author describes how technolg...  \n",
       "16589  Dear Principle,\\n\\nI personally do not think s...  \n",
       "...                                                  ...  \n",
       "3015   When assigned a project during summer break, d...  \n",
       "26587  Dear, State Sentor\\n\\nI think the electoral co...  \n",
       "19477  Dear Floridas state senator, I am righting thi...  \n",
       "30083  Dear senator,\\n\\nGetting ride of the Electoral...  \n",
       "7207   The face is a landform because the planet must...  \n",
       "\n",
       "[29412 rows x 5 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text features only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classic ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LogReg\n",
    "\n",
    "No scaling\n",
    "Min loss \t0.7707357995019517 <br>\n",
    "Max loss \t0.7939273816408711 <br>\n",
    "Mean loss\t0.7830213020862644 <br>\n",
    "\n",
    "\n",
    "\n",
    "Scaled w/ MaxAbsScaler\n",
    "Min loss \t0.8006261104803448 <br>\n",
    "Max loss \t0.8303430962728789 <br>\n",
    "Mean loss\t0.8157244737733116 <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/08/10 19:33:56 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '638d321d45b74353a961073a0dfcafa9', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/experimental/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "2022/08/10 19:33:58 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/opt/homebrew/Caskroom/miniforge/base/envs/experimental/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator.\"\n",
      "2022/08/10 19:33:58 WARNING mlflow.sklearn: Failed to infer model signature: Expected one of (pandas.DataFrame, numpy array, dictionary of (name -> numpy.ndarray), pyspark.sql.DataFrame) but got '<class 'scipy.sparse._csr.csr_matrix'>'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.7869757143697128\n",
      "................................................................................\n",
      "\n",
      "Fold # 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/08/10 19:34:01 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'ae5b9e6c135048d5aa33a40fa5259b82', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/experimental/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "2022/08/10 19:34:02 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/opt/homebrew/Caskroom/miniforge/base/envs/experimental/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator.\"\n",
      "2022/08/10 19:34:02 WARNING mlflow.sklearn: Failed to infer model signature: Expected one of (pandas.DataFrame, numpy array, dictionary of (name -> numpy.ndarray), pyspark.sql.DataFrame) but got '<class 'scipy.sparse._csr.csr_matrix'>'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.7804193422274297\n",
      "................................................................................\n",
      "\n",
      "Fold # 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/08/10 19:34:04 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '6e4687e4886b4058878ca20c93e3abff', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/experimental/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "2022/08/10 19:34:06 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/opt/homebrew/Caskroom/miniforge/base/envs/experimental/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator.\"\n",
      "2022/08/10 19:34:06 WARNING mlflow.sklearn: Failed to infer model signature: Expected one of (pandas.DataFrame, numpy array, dictionary of (name -> numpy.ndarray), pyspark.sql.DataFrame) but got '<class 'scipy.sparse._csr.csr_matrix'>'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.7939273816408711\n",
      "................................................................................\n",
      "\n",
      "Fold # 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/08/10 19:34:08 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'aa7c12f16ef245a3b53bf7a93ae34c35', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/experimental/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "2022/08/10 19:34:09 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/opt/homebrew/Caskroom/miniforge/base/envs/experimental/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator.\"\n",
      "2022/08/10 19:34:09 WARNING mlflow.sklearn: Failed to infer model signature: Expected one of (pandas.DataFrame, numpy array, dictionary of (name -> numpy.ndarray), pyspark.sql.DataFrame) but got '<class 'scipy.sparse._csr.csr_matrix'>'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.7830482726913571\n",
      "................................................................................\n",
      "\n",
      "Fold # 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/08/10 19:34:12 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '57204076de6b4808bb46ba8d7c5a7bea', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/experimental/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "2022/08/10 19:34:13 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/opt/homebrew/Caskroom/miniforge/base/envs/experimental/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator.\"\n",
      "2022/08/10 19:34:13 WARNING mlflow.sklearn: Failed to infer model signature: Expected one of (pandas.DataFrame, numpy array, dictionary of (name -> numpy.ndarray), pyspark.sql.DataFrame) but got '<class 'scipy.sparse._csr.csr_matrix'>'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.7707357995019517\n",
      "................................................................................\n",
      "--------------------------------------------------------------------------------\n",
      "Min loss \t0.7707357995019517\n",
      "Max loss \t0.7939273816408711\n",
      "Mean loss\t0.7830213020862644\n"
     ]
    }
   ],
   "source": [
    "stratified_k = StratifiedKFold(n_splits = 5, shuffle=True, random_state = RANDOM_SEED)\n",
    "losses = []\n",
    "for idx, (train_idxs, valid_idxs) in enumerate(stratified_k.split(X_train, Y_train)):\n",
    "    print(f\"\\nFold # {idx + 1}\")\n",
    "\n",
    "    x_train, y_train = X_train.iloc[train_idxs], Y_train.iloc[train_idxs]\n",
    "    x_valid, y_valid = X_train.iloc[valid_idxs], Y_train.iloc[valid_idxs]\n",
    "\n",
    "    y_train = y_train.to_numpy()\n",
    "    y_valid = y_valid.to_numpy()\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(X_train.essay_text.drop_duplicates())\n",
    "\n",
    "    vectorized_discourse_text_train = vectorizer.transform(x_train.discourse_text)\n",
    "    vectorized_discourse_text_test = vectorizer.transform(x_valid.discourse_text)\n",
    "\n",
    "    ohe = OneHotEncoder()\n",
    "    ohe_discourse_type_train = ohe.fit_transform(x_train.discourse_type.values.reshape(-1, 1))\n",
    "    ohe_discourse_type_test = ohe.transform(x_valid.discourse_type.values.reshape(-1, 1))\n",
    "\n",
    "    x_train = sparse.hstack([vectorized_discourse_text_train, ohe_discourse_type_train])\n",
    "    x_valid = sparse.hstack([vectorized_discourse_text_test, ohe_discourse_type_test])\n",
    "\n",
    "    # In case it is necessary\n",
    "    # scaler = MaxAbsScaler()\n",
    "    # x_train = scaler.fit_transform(x_train)\n",
    "    # x_valid = scaler.transform(x_valid)\n",
    "\n",
    "    model = LogisticRegression()\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    prob_predictions = model.predict_proba(x_valid)\n",
    "    lloss = log_loss(y_valid, prob_predictions)\n",
    "    losses.append(lloss)\n",
    "\n",
    "    print(f\"\\tLog loss: {lloss}\")\n",
    "    print(80*'.')\n",
    "\n",
    "print(80*'-')\n",
    "\n",
    "print(f\"Min loss \\t{min(losses)}\")\n",
    "print(f\"Max loss \\t{max(losses)}\")\n",
    "print(f\"Mean loss\\t{np.mean(losses)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RF\n",
    "\n",
    "Min loss \t0.8211464183621852 <br>\n",
    "Max loss \t0.8326483703178386 <br>\n",
    "Mean loss\t0.8274193801826459 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold # 1\n",
      "\tLog loss: 0.8179767168529476\n",
      "................................................................................\n",
      "\n",
      "Fold # 2\n",
      "\tLog loss: 0.8121509848801739\n",
      "................................................................................\n",
      "\n",
      "Fold # 3\n",
      "\tLog loss: 0.8203104172192276\n",
      "................................................................................\n",
      "\n",
      "Fold # 4\n",
      "\tLog loss: 0.8119259143806231\n",
      "................................................................................\n",
      "\n",
      "Fold # 5\n",
      "\tLog loss: 0.807629720255935\n",
      "................................................................................\n",
      "--------------------------------------------------------------------------------\n",
      "Min loss \t0.807629720255935\n",
      "Max loss \t0.8203104172192276\n",
      "Mean loss\t0.8139987507177814\n"
     ]
    }
   ],
   "source": [
    "stratified_k = StratifiedKFold(n_splits = 5, shuffle=True, random_state = RANDOM_SEED)\n",
    "losses = []\n",
    "for idx, (train_idxs, valid_idxs) in enumerate(stratified_k.split(X_train, Y_train)):\n",
    "    print(f\"\\nFold # {idx + 1}\")\n",
    "\n",
    "    x_train, y_train = X_train.iloc[train_idxs], Y_train.iloc[train_idxs]\n",
    "    x_valid, y_valid = X_train.iloc[valid_idxs], Y_train.iloc[valid_idxs]\n",
    "\n",
    "    y_train = y_train.to_numpy()\n",
    "    y_valid = y_valid.to_numpy()\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(X_train.essay_text.drop_duplicates())\n",
    "\n",
    "    vectorized_discourse_text_train = vectorizer.transform(x_train.discourse_text)\n",
    "    vectorized_discourse_text_test = vectorizer.transform(x_valid.discourse_text)\n",
    "\n",
    "    ohe = OneHotEncoder()\n",
    "    ohe_discourse_type_train = ohe.fit_transform(x_train.discourse_type.values.reshape(-1, 1))\n",
    "    ohe_discourse_type_test = ohe.transform(x_valid.discourse_type.values.reshape(-1, 1))\n",
    "\n",
    "    x_train = sparse.hstack([vectorized_discourse_text_train, ohe_discourse_type_train])\n",
    "    x_valid = sparse.hstack([vectorized_discourse_text_test, ohe_discourse_type_test])\n",
    "\n",
    "    # In case it is necessary\n",
    "    # scaler = MaxAbsScaler()\n",
    "    # x_train = scaler.fit_transform(x_train)\n",
    "    # x_valid = scaler.transform(x_valid)\n",
    "\n",
    "    model = RandomForestClassifier(n_estimators=1000)\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    prob_predictions = model.predict_proba(x_valid)\n",
    "    lloss = log_loss(y_valid, prob_predictions)\n",
    "    losses.append(lloss)\n",
    "\n",
    "    print(f\"\\tLog loss: {lloss}\")\n",
    "    print(80*'.')\n",
    "\n",
    "print(80*'-')\n",
    "\n",
    "print(f\"Min loss \\t{min(losses)}\")\n",
    "print(f\"Max loss \\t{max(losses)}\")\n",
    "print(f\"Mean loss\\t{np.mean(losses)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ExtraTrees\n",
    "Min loss \t0.8255913481053635 <br>\n",
    "Max loss \t0.8682496106013873 <br>\n",
    "Mean loss\t0.8445679909453429"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold # 1\n",
      "\tLog loss: 0.8607272202243996\n",
      "................................................................................\n",
      "\n",
      "Fold # 2\n",
      "\tLog loss: 0.8360894983524675\n",
      "................................................................................\n",
      "\n",
      "Fold # 3\n",
      "\tLog loss: 0.8682496106013873\n",
      "................................................................................\n",
      "\n",
      "Fold # 4\n",
      "\tLog loss: 0.8255913481053635\n",
      "................................................................................\n",
      "\n",
      "Fold # 5\n",
      "\tLog loss: 0.8321822774430964\n",
      "................................................................................\n",
      "--------------------------------------------------------------------------------\n",
      "Min loss \t0.8255913481053635\n",
      "Max loss \t0.8682496106013873\n",
      "Mean loss\t0.8445679909453429\n"
     ]
    }
   ],
   "source": [
    "stratified_k = StratifiedKFold(n_splits = 5, shuffle=True, random_state = RANDOM_SEED)\n",
    "losses = []\n",
    "for idx, (train_idxs, valid_idxs) in enumerate(stratified_k.split(X_train, Y_train)):\n",
    "    print(f\"\\nFold # {idx + 1}\")\n",
    "\n",
    "    x_train, y_train = X_train.iloc[train_idxs], Y_train.iloc[train_idxs]\n",
    "    x_valid, y_valid = X_train.iloc[valid_idxs], Y_train.iloc[valid_idxs]\n",
    "\n",
    "    y_train = y_train.to_numpy()\n",
    "    y_valid = y_valid.to_numpy()\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(X_train.essay_text.drop_duplicates())\n",
    "\n",
    "    vectorized_discourse_text_train = vectorizer.transform(x_train.discourse_text)\n",
    "    vectorized_discourse_text_test = vectorizer.transform(x_valid.discourse_text)\n",
    "\n",
    "    ohe = OneHotEncoder()\n",
    "    ohe_discourse_type_train = ohe.fit_transform(x_train.discourse_type.values.reshape(-1, 1))\n",
    "    ohe_discourse_type_test = ohe.transform(x_valid.discourse_type.values.reshape(-1, 1))\n",
    "\n",
    "    x_train = sparse.hstack([vectorized_discourse_text_train, ohe_discourse_type_train])\n",
    "    x_valid = sparse.hstack([vectorized_discourse_text_test, ohe_discourse_type_test])\n",
    "\n",
    "    # In case it is necessary\n",
    "    # scaler = MaxAbsScaler()\n",
    "    # x_train = scaler.fit_transform(x_train)\n",
    "    # x_valid = scaler.transform(x_valid)\n",
    "\n",
    "    model = ExtraTreesClassifier(n_jobs=-1)\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    prob_predictions = model.predict_proba(x_valid)\n",
    "    lloss = log_loss(y_valid, prob_predictions)\n",
    "    losses.append(lloss)\n",
    "\n",
    "    print(f\"\\tLog loss: {lloss}\")\n",
    "    print(80*'.')\n",
    "\n",
    "print(80*'-')\n",
    "\n",
    "print(f\"Min loss \\t{min(losses)}\")\n",
    "print(f\"Max loss \\t{max(losses)}\")\n",
    "print(f\"Mean loss\\t{np.mean(losses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stratified_k = StratifiedKFold(n_splits = 5, shuffle=True, random_state = RANDOM_SEED)\n",
    "losses = []\n",
    "for idx, (train_idxs, valid_idxs) in enumerate(stratified_k.split(X_train, Y_train)):\n",
    "    print(f\"\\nFold # {idx + 1}\")\n",
    "\n",
    "    x_train, y_train = X_train.iloc[train_idxs], Y_train.iloc[train_idxs]\n",
    "    x_valid, y_valid = X_train.iloc[valid_idxs], Y_train.iloc[valid_idxs]\n",
    "\n",
    "    y_train = y_train.to_numpy()\n",
    "    y_valid = y_valid.to_numpy()\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(X_train.essay_text.drop_duplicates())\n",
    "\n",
    "    vectorized_discourse_text_train = vectorizer.transform(x_train.discourse_text)\n",
    "    vectorized_discourse_text_test = vectorizer.transform(x_valid.discourse_text)\n",
    "\n",
    "    ohe = OneHotEncoder()\n",
    "    ohe_discourse_type_train = ohe.fit_transform(x_train.discourse_type.values.reshape(-1, 1))\n",
    "    ohe_discourse_type_test = ohe.transform(x_valid.discourse_type.values.reshape(-1, 1))\n",
    "\n",
    "    x_train = sparse.hstack([vectorized_discourse_text_train, ohe_discourse_type_train])\n",
    "    x_valid = sparse.hstack([vectorized_discourse_text_test, ohe_discourse_type_test])\n",
    "\n",
    "    # In case it is necessary\n",
    "    # scaler = MaxAbsScaler()\n",
    "    # x_train = scaler.fit_transform(x_train)\n",
    "    # x_valid = scaler.transform(x_valid)\n",
    "\n",
    "    model = IsolationForest(n_jobs=-1)\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    prob_predictions = model.predict_proba(x_valid)\n",
    "    lloss = log_loss(y_valid, prob_predictions)\n",
    "    losses.append(lloss)\n",
    "\n",
    "    print(f\"\\tLog loss: {lloss}\")\n",
    "    print(80*'.')\n",
    "\n",
    "print(80*'-')\n",
    "\n",
    "print(f\"Min loss \\t{min(losses)}\")\n",
    "print(f\"Max loss \\t{max(losses)}\")\n",
    "print(f\"Mean loss\\t{np.mean(losses)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost\n",
    "Min loss \t0.769898167914014 <br>\n",
    "Max loss \t0.7861944754332157 <br>\n",
    "Mean loss\t0.7808887629128117 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/08/10 19:26:25 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '85b98ebfe07c4f72811640207eac6e0f', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current xgboost workflow\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/experimental/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:26:26] WARNING: /private/var/folders/c6/5pqpts4j0z714gbd4lgp_1g80000gn/T/pip-install-v5ts534i/xgboost_d6868f90e2e54d9f93ddde7d76574102/build/temp.macosx-11.0-arm64-3.9/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/08/10 19:27:12 WARNING mlflow.sklearn: Failed to infer model signature: Expected one of (pandas.DataFrame, numpy array, dictionary of (name -> numpy.ndarray), pyspark.sql.DataFrame) but got '<class 'scipy.sparse._csr.csr_matrix'>'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.7831609343908684\n",
      "................................................................................\n",
      "\n",
      "Fold # 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/08/10 19:27:15 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '2d0aa38649744f759bd89ba849c8939c', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current xgboost workflow\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/experimental/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:27:15] WARNING: /private/var/folders/c6/5pqpts4j0z714gbd4lgp_1g80000gn/T/pip-install-v5ts534i/xgboost_d6868f90e2e54d9f93ddde7d76574102/build/temp.macosx-11.0-arm64-3.9/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/08/10 19:28:02 WARNING mlflow.sklearn: Failed to infer model signature: Expected one of (pandas.DataFrame, numpy array, dictionary of (name -> numpy.ndarray), pyspark.sql.DataFrame) but got '<class 'scipy.sparse._csr.csr_matrix'>'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.7817585589293724\n",
      "................................................................................\n",
      "\n",
      "Fold # 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/08/10 19:28:04 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '17153cd8e63a4ec899b8ea9934ed18bd', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current xgboost workflow\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/experimental/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:28:05] WARNING: /private/var/folders/c6/5pqpts4j0z714gbd4lgp_1g80000gn/T/pip-install-v5ts534i/xgboost_d6868f90e2e54d9f93ddde7d76574102/build/temp.macosx-11.0-arm64-3.9/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/08/10 19:28:51 WARNING mlflow.sklearn: Failed to infer model signature: Expected one of (pandas.DataFrame, numpy array, dictionary of (name -> numpy.ndarray), pyspark.sql.DataFrame) but got '<class 'scipy.sparse._csr.csr_matrix'>'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.7834316778965883\n",
      "................................................................................\n",
      "\n",
      "Fold # 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/08/10 19:28:54 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'f2809c6dbda04f99977e906af55cc29d', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current xgboost workflow\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/experimental/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:28:54] WARNING: /private/var/folders/c6/5pqpts4j0z714gbd4lgp_1g80000gn/T/pip-install-v5ts534i/xgboost_d6868f90e2e54d9f93ddde7d76574102/build/temp.macosx-11.0-arm64-3.9/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/08/10 19:29:41 WARNING mlflow.sklearn: Failed to infer model signature: Expected one of (pandas.DataFrame, numpy array, dictionary of (name -> numpy.ndarray), pyspark.sql.DataFrame) but got '<class 'scipy.sparse._csr.csr_matrix'>'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.7861944754332157\n",
      "................................................................................\n",
      "\n",
      "Fold # 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/08/10 19:29:43 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '9eecbea3e7c749559c4836195ed9217c', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current xgboost workflow\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/experimental/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:29:44] WARNING: /private/var/folders/c6/5pqpts4j0z714gbd4lgp_1g80000gn/T/pip-install-v5ts534i/xgboost_d6868f90e2e54d9f93ddde7d76574102/build/temp.macosx-11.0-arm64-3.9/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/08/10 19:30:31 WARNING mlflow.sklearn: Failed to infer model signature: Expected one of (pandas.DataFrame, numpy array, dictionary of (name -> numpy.ndarray), pyspark.sql.DataFrame) but got '<class 'scipy.sparse._csr.csr_matrix'>'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.769898167914014\n",
      "................................................................................\n",
      "--------------------------------------------------------------------------------\n",
      "Min loss \t0.769898167914014\n",
      "Max loss \t0.7861944754332157\n",
      "Mean loss\t0.7808887629128117\n"
     ]
    }
   ],
   "source": [
    "stratified_k = StratifiedKFold(n_splits = 5, shuffle=True, random_state = RANDOM_SEED)\n",
    "losses = []\n",
    "for idx, (train_idxs, valid_idxs) in enumerate(stratified_k.split(X_train, Y_train)):\n",
    "    print(f\"\\nFold # {idx + 1}\")\n",
    "\n",
    "    x_train, y_train = X_train.iloc[train_idxs], Y_train.iloc[train_idxs]\n",
    "    x_valid, y_valid = X_train.iloc[valid_idxs], Y_train.iloc[valid_idxs]\n",
    "\n",
    "    y_train = y_train.to_numpy()\n",
    "    y_valid = y_valid.to_numpy()\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(X_train.essay_text.drop_duplicates())\n",
    "\n",
    "    vectorized_discourse_text_train = vectorizer.transform(x_train.discourse_text)\n",
    "    vectorized_discourse_text_test = vectorizer.transform(x_valid.discourse_text)\n",
    "\n",
    "    ohe = OneHotEncoder()\n",
    "    ohe_discourse_type_train = ohe.fit_transform(x_train.discourse_type.values.reshape(-1, 1))\n",
    "    ohe_discourse_type_test = ohe.transform(x_valid.discourse_type.values.reshape(-1, 1))\n",
    "\n",
    "    x_train = sparse.hstack([vectorized_discourse_text_train, ohe_discourse_type_train])\n",
    "    x_valid = sparse.hstack([vectorized_discourse_text_test, ohe_discourse_type_test])\n",
    "\n",
    "    # In case it is necessary\n",
    "    # scaler = MaxAbsScaler()\n",
    "    # x_train = scaler.fit_transform(x_train)\n",
    "    # x_valid = scaler.transform(x_valid)\n",
    "\n",
    "    model = XGBClassifier()\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    prob_predictions = model.predict_proba(x_valid)\n",
    "    lloss = log_loss(y_valid, prob_predictions)\n",
    "    losses.append(lloss)\n",
    "\n",
    "    print(f\"\\tLog loss: {lloss}\")\n",
    "    print(80*'.')\n",
    "\n",
    "print(80*'-')\n",
    "\n",
    "print(f\"Min loss \\t{min(losses)}\")\n",
    "print(f\"Max loss \\t{max(losses)}\")\n",
    "print(f\"Mean loss\\t{np.mean(losses)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LGBM\n",
    "Min loss \t0.7565350176859863 <br>\n",
    "Max loss \t0.7765817628486158 <br>\n",
    "Mean loss\t0.7692048809033425 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold # 1\n",
      "\tLog loss: 0.7698034071294326\n",
      "................................................................................\n",
      "\n",
      "Fold # 2\n",
      "\tLog loss: 0.7695595489450545\n",
      "................................................................................\n",
      "\n",
      "Fold # 3\n",
      "\tLog loss: 0.7765817628486158\n",
      "................................................................................\n",
      "\n",
      "Fold # 4\n",
      "\tLog loss: 0.773544667907623\n",
      "................................................................................\n",
      "\n",
      "Fold # 5\n",
      "\tLog loss: 0.7565350176859863\n",
      "................................................................................\n",
      "--------------------------------------------------------------------------------\n",
      "Min loss \t0.7565350176859863\n",
      "Max loss \t0.7765817628486158\n",
      "Mean loss\t0.7692048809033425\n"
     ]
    }
   ],
   "source": [
    "stratified_k = StratifiedKFold(n_splits = 5, shuffle=True, random_state = RANDOM_SEED)\n",
    "losses = []\n",
    "for idx, (train_idxs, valid_idxs) in enumerate(stratified_k.split(X_train, Y_train)):\n",
    "    print(f\"\\nFold # {idx + 1}\")\n",
    "\n",
    "    x_train, y_train = X_train.iloc[train_idxs], Y_train.iloc[train_idxs]\n",
    "    x_valid, y_valid = X_train.iloc[valid_idxs], Y_train.iloc[valid_idxs]\n",
    "\n",
    "    y_train = y_train.to_numpy()\n",
    "    y_valid = y_valid.to_numpy()\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(X_train.essay_text.drop_duplicates())\n",
    "\n",
    "    vectorized_discourse_text_train = vectorizer.transform(x_train.discourse_text)\n",
    "    vectorized_discourse_text_test = vectorizer.transform(x_valid.discourse_text)\n",
    "\n",
    "    ohe = OneHotEncoder()\n",
    "    ohe_discourse_type_train = ohe.fit_transform(x_train.discourse_type.values.reshape(-1, 1))\n",
    "    ohe_discourse_type_test = ohe.transform(x_valid.discourse_type.values.reshape(-1, 1))\n",
    "\n",
    "    x_train = sparse.hstack([vectorized_discourse_text_train, ohe_discourse_type_train])\n",
    "    x_valid = sparse.hstack([vectorized_discourse_text_test, ohe_discourse_type_test])\n",
    "\n",
    "    # In case it is necessary\n",
    "    # scaler = MaxAbsScaler()\n",
    "    # x_train = scaler.fit_transform(x_train)\n",
    "    # x_valid = scaler.transform(x_valid)\n",
    "\n",
    "    model = LGBMClassifier()\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    prob_predictions = model.predict_proba(x_valid)\n",
    "    lloss = log_loss(y_valid, prob_predictions)\n",
    "    losses.append(lloss)\n",
    "\n",
    "    print(f\"\\tLog loss: {lloss}\")\n",
    "    print(80*'.')\n",
    "\n",
    "print(80*'-')\n",
    "\n",
    "print(f\"Min loss \\t{min(losses)}\")\n",
    "print(f\"Max loss \\t{max(losses)}\")\n",
    "print(f\"Mean loss\\t{np.mean(losses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7353, 3)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7759086348638186"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = log_loss(y_true = Y_test, y_pred = Y_pred)\n",
    "loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/lightgbm/basic.py:859: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.768475009212139\n",
      "................................................................................\n",
      "\n",
      "Fold # 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/lightgbm/basic.py:859: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.765759454909087\n",
      "................................................................................\n",
      "\n",
      "Fold # 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/lightgbm/basic.py:859: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.7766540270328358\n",
      "................................................................................\n",
      "\n",
      "Fold # 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/lightgbm/basic.py:859: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.7727770030010869\n",
      "................................................................................\n",
      "\n",
      "Fold # 5\n",
      "\tLog loss: 0.7590620222984978\n",
      "................................................................................\n",
      "--------------------------------------------------------------------------------\n",
      "Min loss \t0.7590620222984978\n",
      "Max loss \t0.7766540270328358\n",
      "Mean loss\t0.7685455032907293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/lightgbm/basic.py:859: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    }
   ],
   "source": [
    "stratified_k = StratifiedKFold(n_splits = 5, shuffle=True, random_state = RANDOM_SEED)\n",
    "losses = []\n",
    "for idx, (train_idxs, valid_idxs) in enumerate(stratified_k.split(X_train, Y_train)):\n",
    "    print(f\"\\nFold # {idx + 1}\")\n",
    "\n",
    "    x_train, y_train = X_train.iloc[train_idxs], Y_train.iloc[train_idxs]\n",
    "    x_valid, y_valid = X_train.iloc[valid_idxs], Y_train.iloc[valid_idxs]\n",
    "\n",
    "    y_train = y_train.to_numpy()\n",
    "    y_valid = y_valid.to_numpy()\n",
    "\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "    vectorizer.fit(X_train.essay_text.drop_duplicates())\n",
    "\n",
    "    vectorized_discourse_text_train = vectorizer.transform(x_train.discourse_text)\n",
    "    vectorized_discourse_text_test = vectorizer.transform(x_valid.discourse_text)\n",
    "\n",
    "    ohe = OneHotEncoder()\n",
    "    ohe_discourse_type_train = ohe.fit_transform(x_train.discourse_type.values.reshape(-1, 1))\n",
    "    ohe_discourse_type_test = ohe.transform(x_valid.discourse_type.values.reshape(-1, 1))\n",
    "\n",
    "    x_train = sparse.hstack([vectorized_discourse_text_train, ohe_discourse_type_train])\n",
    "    x_valid = sparse.hstack([vectorized_discourse_text_test, ohe_discourse_type_test])\n",
    "\n",
    "    # In case it is necessary\n",
    "    # scaler = MaxAbsScaler()\n",
    "    # x_train = scaler.fit_transform(x_train)\n",
    "    # x_valid = scaler.transform(x_valid)\n",
    "\n",
    "    model = LGBMClassifier()\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    prob_predictions = model.predict_proba(x_valid)\n",
    "    lloss = log_loss(y_valid, prob_predictions)\n",
    "    losses.append(lloss)\n",
    "\n",
    "    print(f\"\\tLog loss: {lloss}\")\n",
    "    print(80*'.')\n",
    "\n",
    "print(80*'-')\n",
    "\n",
    "print(f\"Min loss \\t{min(losses)}\")\n",
    "print(f\"Max loss \\t{max(losses)}\")\n",
    "print(f\"Mean loss\\t{np.mean(losses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/lightgbm/basic.py:859: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.7719784893139021\n",
      "................................................................................\n",
      "\n",
      "Fold # 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/lightgbm/basic.py:859: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.7659387707507819\n",
      "................................................................................\n",
      "\n",
      "Fold # 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/lightgbm/basic.py:859: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.7764218927406688\n",
      "................................................................................\n",
      "\n",
      "Fold # 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/lightgbm/basic.py:859: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.7770481802053392\n",
      "................................................................................\n",
      "\n",
      "Fold # 5\n",
      "\tLog loss: 0.7606461736088924\n",
      "................................................................................\n",
      "--------------------------------------------------------------------------------\n",
      "Min loss \t0.7606461736088924\n",
      "Max loss \t0.7770481802053392\n",
      "Mean loss\t0.7704067013239169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/lightgbm/basic.py:859: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    }
   ],
   "source": [
    "stratified_k = StratifiedKFold(n_splits = 5, shuffle=True, random_state = RANDOM_SEED)\n",
    "losses = []\n",
    "for idx, (train_idxs, valid_idxs) in enumerate(stratified_k.split(X_train, Y_train)):\n",
    "    print(f\"\\nFold # {idx + 1}\")\n",
    "\n",
    "    x_train, y_train = X_train.iloc[train_idxs], Y_train.iloc[train_idxs]\n",
    "    x_valid, y_valid = X_train.iloc[valid_idxs], Y_train.iloc[valid_idxs]\n",
    "\n",
    "    y_train = y_train.to_numpy()\n",
    "    y_valid = y_valid.to_numpy()\n",
    "\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1,3))\n",
    "    vectorizer.fit(X_train.essay_text.drop_duplicates())\n",
    "\n",
    "    vectorized_discourse_text_train = vectorizer.transform(x_train.discourse_text)\n",
    "    vectorized_discourse_text_test = vectorizer.transform(x_valid.discourse_text)\n",
    "\n",
    "    ohe = OneHotEncoder()\n",
    "    ohe_discourse_type_train = ohe.fit_transform(x_train.discourse_type.values.reshape(-1, 1))\n",
    "    ohe_discourse_type_test = ohe.transform(x_valid.discourse_type.values.reshape(-1, 1))\n",
    "\n",
    "    x_train = sparse.hstack([vectorized_discourse_text_train, ohe_discourse_type_train])\n",
    "    x_valid = sparse.hstack([vectorized_discourse_text_test, ohe_discourse_type_test])\n",
    "\n",
    "    # In case it is necessary\n",
    "    # scaler = MaxAbsScaler()\n",
    "    # x_train = scaler.fit_transform(x_train)\n",
    "    # x_valid = scaler.transform(x_valid)\n",
    "\n",
    "    model = LGBMClassifier()\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    prob_predictions = model.predict_proba(x_valid)\n",
    "    lloss = log_loss(y_valid, prob_predictions)\n",
    "    losses.append(lloss)\n",
    "\n",
    "    print(f\"\\tLog loss: {lloss}\")\n",
    "    print(80*'.')\n",
    "\n",
    "print(80*'-')\n",
    "\n",
    "print(f\"Min loss \\t{min(losses)}\")\n",
    "print(f\"Max loss \\t{max(losses)}\")\n",
    "print(f\"Mean loss\\t{np.mean(losses)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently, slightly worse than n_gram_range(1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HistGradientBoosting\n",
    "Min loss \t0.7589378423671769 <br>\n",
    "Max loss \t0.7814936649694296 <br>\n",
    "Mean loss\t0.7706771662759752 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.7725146625309408\n",
      "................................................................................\n",
      "\n",
      "Fold # 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.7672879132724585\n",
      "................................................................................\n",
      "\n",
      "Fold # 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.7803359314651005\n",
      "................................................................................\n",
      "\n",
      "Fold # 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.7760577902829843\n",
      "................................................................................\n",
      "\n",
      "Fold # 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.7572928859563774\n",
      "................................................................................\n",
      "--------------------------------------------------------------------------------\n",
      "Min loss \t0.7572928859563774\n",
      "Max loss \t0.7803359314651005\n",
      "Mean loss\t0.7706978367015722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "stratified_k = StratifiedKFold(n_splits = 5, shuffle=True, random_state = RANDOM_SEED)\n",
    "losses = []\n",
    "for idx, (train_idxs, valid_idxs) in enumerate(stratified_k.split(X_train, Y_train)):\n",
    "    print(f\"\\nFold # {idx + 1}\")\n",
    "\n",
    "    x_train, y_train = X_train.iloc[train_idxs], Y_train.iloc[train_idxs]\n",
    "    x_valid, y_valid = X_train.iloc[valid_idxs], Y_train.iloc[valid_idxs]\n",
    "\n",
    "    y_train = y_train.to_numpy()\n",
    "    y_valid = y_valid.to_numpy()\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(X_train.essay_text.drop_duplicates())\n",
    "\n",
    "    vectorized_discourse_text_train = vectorizer.transform(x_train.discourse_text)\n",
    "    vectorized_discourse_text_test = vectorizer.transform(x_valid.discourse_text)\n",
    "\n",
    "    ohe = OneHotEncoder()\n",
    "    ohe_discourse_type_train = ohe.fit_transform(x_train.discourse_type.values.reshape(-1, 1))\n",
    "    ohe_discourse_type_test = ohe.transform(x_valid.discourse_type.values.reshape(-1, 1))\n",
    "\n",
    "    x_train = sparse.hstack([vectorized_discourse_text_train, ohe_discourse_type_train])\n",
    "    x_valid = sparse.hstack([vectorized_discourse_text_test, ohe_discourse_type_test])\n",
    "\n",
    "    # In case it is necessary\n",
    "    # scaler = MaxAbsScaler()\n",
    "    # x_train = scaler.fit_transform(x_train)\n",
    "    # x_valid = scaler.transform(x_valid)\n",
    "\n",
    "    x_train, x_valid = x_train.todense(), x_valid.todense()\n",
    "\n",
    "    model = HistGradientBoostingClassifier()\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    prob_predictions = model.predict_proba(x_valid)\n",
    "    lloss = log_loss(y_valid, prob_predictions)\n",
    "    losses.append(lloss)\n",
    "\n",
    "    print(f\"\\tLog loss: {lloss}\")\n",
    "    print(80*'.')\n",
    "\n",
    "print(80*'-')\n",
    "\n",
    "print(f\"Min loss \\t{min(losses)}\")\n",
    "print(f\"Max loss \\t{max(losses)}\")\n",
    "print(f\"Mean loss\\t{np.mean(losses)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, CategoricalNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 1.0052641991904334\n",
      "................................................................................\n",
      "\n",
      "Fold # 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.9989523898046153\n",
      "................................................................................\n",
      "\n",
      "Fold # 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 1.0012554263271314\n",
      "................................................................................\n",
      "\n",
      "Fold # 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 1.00147215303088\n",
      "................................................................................\n",
      "\n",
      "Fold # 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.9826888223621003\n",
      "................................................................................\n",
      "--------------------------------------------------------------------------------\n",
      "Min loss \t0.9826888223621003\n",
      "Max loss \t1.0052641991904334\n",
      "Mean loss\t0.9979265981430322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "stratified_k = StratifiedKFold(n_splits = 5, shuffle=True, random_state = RANDOM_SEED)\n",
    "losses = []\n",
    "for idx, (train_idxs, valid_idxs) in enumerate(stratified_k.split(X_train, Y_train)):\n",
    "    print(f\"\\nFold # {idx + 1}\")\n",
    "\n",
    "    x_train, y_train = X_train.iloc[train_idxs], Y_train.iloc[train_idxs]\n",
    "    x_valid, y_valid = X_train.iloc[valid_idxs], Y_train.iloc[valid_idxs]\n",
    "\n",
    "    y_train = y_train.to_numpy()\n",
    "    y_valid = y_valid.to_numpy()\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(X_train.essay_text.drop_duplicates())\n",
    "\n",
    "    vectorized_discourse_text_train = vectorizer.transform(x_train.discourse_text)\n",
    "    vectorized_discourse_text_test = vectorizer.transform(x_valid.discourse_text)\n",
    "\n",
    "    ohe = OneHotEncoder()\n",
    "    ohe_discourse_type_train = ohe.fit_transform(x_train.discourse_type.values.reshape(-1, 1))\n",
    "    ohe_discourse_type_test = ohe.transform(x_valid.discourse_type.values.reshape(-1, 1))\n",
    "\n",
    "    x_train = sparse.hstack([vectorized_discourse_text_train, ohe_discourse_type_train])\n",
    "    x_valid = sparse.hstack([vectorized_discourse_text_test, ohe_discourse_type_test])\n",
    "\n",
    "    # In case it is necessary\n",
    "    # scaler = MaxAbsScaler()\n",
    "    # x_train = scaler.fit_transform(x_train)\n",
    "    # x_valid = scaler.transform(x_valid)\n",
    "\n",
    "    x_train, x_valid = x_train.todense(), x_valid.todense()\n",
    "\n",
    "    model = MultinomialNB()\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    prob_predictions = model.predict_proba(x_valid)\n",
    "    lloss = log_loss(y_valid, prob_predictions)\n",
    "    losses.append(lloss)\n",
    "\n",
    "    print(f\"\\tLog loss: {lloss}\")\n",
    "    print(80*'.')\n",
    "\n",
    "print(80*'-')\n",
    "\n",
    "print(f\"Min loss \\t{min(losses)}\")\n",
    "print(f\"Max loss \\t{max(losses)}\")\n",
    "print(f\"Mean loss\\t{np.mean(losses)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "w/ more data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.9541224343739063\n",
      "................................................................................\n",
      "--------------------------------------------------------------------------------\n",
      "Min loss \t0.9541224343739063\n",
      "Max loss \t0.9541224343739063\n",
      "Mean loss\t0.9541224343739063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "x_train, y_train = X_train, Y_train\n",
    "x_valid, y_valid = X_test, Y_test\n",
    "\n",
    "y_train = y_train.to_numpy()\n",
    "y_valid = y_valid.to_numpy()\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(X_train.essay_text.drop_duplicates())\n",
    "\n",
    "vectorized_discourse_text_train = vectorizer.transform(x_train.discourse_text)\n",
    "vectorized_discourse_text_test = vectorizer.transform(x_valid.discourse_text)\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "ohe_discourse_type_train = ohe.fit_transform(x_train.discourse_type.values.reshape(-1, 1))\n",
    "ohe_discourse_type_test = ohe.transform(x_valid.discourse_type.values.reshape(-1, 1))\n",
    "\n",
    "x_train = sparse.hstack([vectorized_discourse_text_train, ohe_discourse_type_train])\n",
    "x_valid = sparse.hstack([vectorized_discourse_text_test, ohe_discourse_type_test])\n",
    "\n",
    "# In case it is necessary\n",
    "# scaler = MaxAbsScaler()\n",
    "# x_train = scaler.fit_transform(x_train)\n",
    "# x_valid = scaler.transform(x_valid)\n",
    "\n",
    "x_train, x_valid = x_train.todense(), x_valid.todense()\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "prob_predictions = model.predict_proba(x_valid)\n",
    "lloss = log_loss(y_valid, prob_predictions)\n",
    "losses.append(lloss)\n",
    "\n",
    "print(f\"\\tLog loss: {lloss}\")\n",
    "print(80*'.')\n",
    "\n",
    "print(80*'-')\n",
    "\n",
    "print(f\"Min loss \\t{min(losses)}\")\n",
    "print(f\"Max loss \\t{max(losses)}\")\n",
    "print(f\"Mean loss\\t{np.mean(losses)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/tf_env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 512\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[SEP]'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sep_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(sentences):\n",
    "    return tokenizer(sentences, truncation = True, add_special_tokens=True, padding = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'StringMethods' object has no attribute 'map'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/hdd_n/Desktop/hnnaharendt/feedback-kaggle/experimentation_CPU.ipynb Cell 52\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/hdd_n/Desktop/hnnaharendt/feedback-kaggle/experimentation_CPU.ipynb#Y102sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m X_train_ \u001b[39m=\u001b[39m X_train[\u001b[39m'\u001b[39;49m\u001b[39mdiscourse_text\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mstr\u001b[39m.\u001b[39;49mmap(tokenize_data)\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hdd_n/Desktop/hnnaharendt/feedback-kaggle/experimentation_CPU.ipynb#Y102sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m X_test_ \u001b[39m=\u001b[39m X_test[\u001b[39m'\u001b[39m\u001b[39mdiscourse_text\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mmap(tokenize_data)\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hdd_n/Desktop/hnnaharendt/feedback-kaggle/experimentation_CPU.ipynb#Y102sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m Y_train_\u001b[39m=\u001b[39m Y_train\u001b[39m.\u001b[39mtolist()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'StringMethods' object has no attribute 'map'"
     ]
    }
   ],
   "source": [
    "X_train_ = X_train['discourse_text'].str.map(tokenize_data).tolist()\n",
    "X_test_ = X_test['discourse_text'].map(tokenize_data).tolist()\n",
    "Y_train_= Y_train.tolist()\n",
    "Y_test_ = Y_test.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-17 15:15:27.139028: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-08-17 15:15:28.309711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-17 15:15:28.312927: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-17 15:15:28.313036: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-17 15:15:28.313349: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-08-17 15:15:28.313916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-17 15:15:28.314017: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-17 15:15:28.314088: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-17 15:15:28.611169: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-17 15:15:28.611289: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-17 15:15:28.611358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-17 15:15:28.611425: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21156 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2022-08-17 15:15:29.163884: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForSequenceClassification.from_pretrained('bert-base-cased', num_labels = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  108310272 \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  2307      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 108,312,579\n",
      "Trainable params: 108,312,579\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'transformers.tokenization_utils_base.BatchEncoding'>\"}), (<class 'list'> containing values of types {\"<class 'int'>\"})",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/hdd_n/Desktop/hnnaharendt/feedback-kaggle/experimentation_CPU.ipynb Cell 57\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/hdd_n/Desktop/hnnaharendt/feedback-kaggle/experimentation_CPU.ipynb#Y113sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train_, Y_train_)\n",
      "File \u001b[0;32m~/mambaforge/envs/tf_env/lib/python3.10/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/mambaforge/envs/tf_env/lib/python3.10/site-packages/keras/engine/data_adapter.py:985\u001b[0m, in \u001b[0;36mselect_data_adapter\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    982\u001b[0m adapter_cls \u001b[39m=\u001b[39m [\u001b[39mcls\u001b[39m \u001b[39mfor\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39min\u001b[39;00m ALL_ADAPTER_CLS \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mcan_handle(x, y)]\n\u001b[1;32m    983\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m adapter_cls:\n\u001b[1;32m    984\u001b[0m   \u001b[39m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[39;00m\n\u001b[0;32m--> 985\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    986\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mFailed to find data adapter that can handle \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    987\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39minput: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    988\u001b[0m           _type_name(x), _type_name(y)))\n\u001b[1;32m    989\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(adapter_cls) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    990\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    991\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mData adapters should be mutually exclusive for \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    992\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mhandling inputs. Found multiple adapters \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m to handle \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    993\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39minput: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    994\u001b[0m           adapter_cls, _type_name(x), _type_name(y)))\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'transformers.tokenization_utils_base.BatchEncoding'>\"}), (<class 'list'> containing values of types {\"<class 'int'>\"})"
     ]
    }
   ],
   "source": [
    "model.fit(X_train_, Y_train_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29412,)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shallow features (word-count, syllables, sentences, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "DICTIONARY = Dictionary.from_files('en_US')\n",
    "PUNCTUATIONS = set(list(punctuation))\n",
    "PUNCTUATIONS.update(\"`\")\n",
    "PUNCTUATIONS.update(\"'\")\n",
    "CONTRACTIONS = contractions.contractions_dict\n",
    "CONTRACTIONS[\"It'll\"] = \"It will\"\n",
    "CONTRACTIONS = {key.capitalize(): value for key, value in CONTRACTIONS.items()}\n",
    "\n",
    "\n",
    "def check_word(token):\n",
    "    if DICTIONARY.lookup(token):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def replace_contractions(text):\n",
    "\n",
    "    \n",
    "    for key, value in CONTRACTIONS.items():\n",
    "        # Upper-case\n",
    "        text = text.replace(key, value)\n",
    "        # Lower-case \n",
    "        text = text.replace(key.lower(), value.lower())\n",
    "\n",
    "    # Remove possesives as well \n",
    "    text = text.replace(\"'s\", \"\")\n",
    "\n",
    "    # Remove unnecessary whitespaces\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    \n",
    "    return text \n",
    "\n",
    "NOT_MISTAKES = ['landform', 'driverless']\n",
    "\n",
    "def remove_empty_tokens(list_of_tokens):\n",
    "    list_of_tokens = list(filter(None, list_of_tokens))\n",
    "    return list_of_tokens\n",
    "\n",
    "def check_in_dictionary(sentences):\n",
    "\n",
    "    # If the contraction can be removed, than it means it's correct; consequently, only possibly incorrect words remain\n",
    "\n",
    "    tokenized_sentences = word_tokenize(sentences)\n",
    "    correct_tokens = [check_word(token) if (token not in PUNCTUATIONS) and (token not in NOT_MISTAKES)  else 1 for token in tokenized_sentences]\n",
    "\n",
    "    return correct_tokens\n",
    "\n",
    "def get_incorrect_indices(correct_word_list):\n",
    "    return [idx for idx, value in enumerate(correct_word_list) if not value]\n",
    "\n",
    "def get_incorrect_words(words_list, indices):\n",
    "    return [words_list[idx] for idx in indices if words_list[idx] ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_relative_position(argument, essay):\n",
    "    essay_length = len(essay)\n",
    "    split_essay = essay.split(argument.strip())\n",
    "    \n",
    "    if split_essay[0] == essay:\n",
    "        return (0,0)\n",
    "\n",
    "    if split_essay ==  ['', '']:\n",
    "        return (0,1)\n",
    "    \n",
    "    if split_essay[0] == '':\n",
    "        return (0, (essay_length - len(split_essay[1])) / essay_length)\n",
    "    \n",
    "    try:\n",
    "        if split_essay[1] == '':  \n",
    "            return ((essay_length - len(split_essay[0])) / essay_length, 1)\n",
    "    except:\n",
    "        return None\n",
    "    else:\n",
    "        return (\n",
    "            len(split_essay[0]) / essay_length,\n",
    "            (len(split_essay[0]) + len(argument)) / essay_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['sentences'] = X_train.discourse_text.map(sent_tokenize)\n",
    "X_train['words'] = X_train.sentences.apply(lambda sentences: list(chain(*[word_tokenize(sentence) for sentence in sentences])))\n",
    "X_train['words'] = X_train.words.apply(lambda tokens: [token for token in tokens if (token not in PUNCTUATIONS and token != '')])\n",
    "X_train['syllables'] = X_train.words.apply(lambda tokens: list(chain(*[hyphenate_word(token) for token in tokens])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['text_'] = X_train.discourse_text.map(replace_contractions)\n",
    "X_train['tokenized_sentences'] = X_train.text_.map(word_tokenize)\n",
    "X_train['correct_list'] = X_train.text_.map(check_in_dictionary)\n",
    "X_train['incorrect_indices'] = X_train.correct_list.map(get_incorrect_indices)\n",
    "X_train['incorrect_words'] = X_train.apply(lambda x: get_incorrect_words(x.tokenized_sentences, x.incorrect_indices), axis = 1)\n",
    "\n",
    "X_train['incorrect_words_len'] = X_train.incorrect_words.map(len)\n",
    "X_train['relative_position'] = X_train.apply(lambda x: find_relative_position(argument = x.discourse_text, essay = x.essay_text), axis = 1)\n",
    "X_train[['rp_start','rp_end']] = pd.DataFrame(X_train['relative_position'].tolist(), index = X_train.index)\n",
    "\n",
    "X_train['sent_count'] = X_train.sentences.map(len)\n",
    "X_train['word_count'] = X_train.words.map(len)\n",
    "X_train['syll_count'] = X_train.syllables.map(len)\n",
    "\n",
    "X_train['words_per_sentences'] = X_train['sent_count'] / X_train['word_count']\n",
    "X_train['syll_per_sentences'] = X_train['sent_count'] / X_train['syll_count'] \n",
    "X_train['syll_per_words'] = X_train['word_count'] / X_train['syll_count']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(max_iter = 1000)\n",
    "xgb = XGBClassifier()\n",
    "lgbm = LGBMClassifier()\n",
    "extra = ExtraTreesClassifier(n_jobs=-1)\n",
    "hgbm = HistGradientBoostingClassifier()\n",
    "dt = DecisionTreeClassifier()\n",
    "rf = RandomForestClassifier(n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [lr, dt, rf, xgb, lgbm, extra]\n",
    "models_2 = [hgbm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "LogisticRegression\n",
      "\n",
      "LogisticRegression(max_iter=1000)\n",
      "\n",
      "Fold # 1\n",
      "\tLog loss: 0.7721808983819242\n",
      "\tAccuracy: 0.6561278259391466\n",
      "\tMCC: 0.3564077227754731\n",
      "................................................................................\n",
      "\n",
      "Fold # 2\n",
      "\tLog loss: 0.7668818205606772\n",
      "\tAccuracy: 0.6562978072412035\n",
      "\tMCC: 0.3564633335267342\n",
      "................................................................................\n",
      "\n",
      "Fold # 3\n",
      "\tLog loss: 0.7733801027402161\n",
      "\tAccuracy: 0.6521591295477729\n",
      "\tMCC: 0.34763770809798955\n",
      "................................................................................\n",
      "\n",
      "Fold # 4\n",
      "\tLog loss: 0.7655158225723563\n",
      "\tAccuracy: 0.6591295477728664\n",
      "\tMCC: 0.36007840144893005\n",
      "................................................................................\n",
      "\n",
      "Fold # 5\n",
      "\tLog loss: 0.748670305385369\n",
      "\tAccuracy: 0.6725603536212172\n",
      "\tMCC: 0.38913206378702975\n",
      "................................................................................\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Min loss \t0.748670305385369\n",
      "Max loss \t0.7733801027402161\n",
      "Mean loss\t0.7653257899281086\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "DecisionTreeClassifier\n",
      "\n",
      "DecisionTreeClassifier()\n",
      "\n",
      "Fold # 1\n",
      "\tLog loss: 16.274262819427573\n",
      "\tAccuracy: 0.5288118306986231\n",
      "\tMCC: 0.1778233639644859\n",
      "................................................................................\n",
      "\n",
      "Fold # 2\n",
      "\tLog loss: 16.203811465230917\n",
      "\tAccuracy: 0.5308516063233044\n",
      "\tMCC: 0.17787376396637714\n",
      "................................................................................\n",
      "\n",
      "Fold # 3\n",
      "\tLog loss: 15.46670129619088\n",
      "\tAccuracy: 0.5521931315878953\n",
      "\tMCC: 0.20386930553318008\n",
      "................................................................................\n",
      "\n",
      "Fold # 4\n",
      "\tLog loss: 16.03040794935501\n",
      "\tAccuracy: 0.5358721523291398\n",
      "\tMCC: 0.17781607460720705\n",
      "................................................................................\n",
      "\n",
      "Fold # 5\n",
      "\tLog loss: 15.543036572140187\n",
      "\tAccuracy: 0.5499829989799387\n",
      "\tMCC: 0.20394184020150333\n",
      "................................................................................\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Min loss \t15.46670129619088\n",
      "Max loss \t16.274262819427573\n",
      "Mean loss\t15.903644020468912\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "RandomForestClassifier\n",
      "\n",
      "RandomForestClassifier(n_jobs=-1)\n",
      "\n",
      "Fold # 1\n",
      "\tLog loss: 0.8221255433609445\n",
      "\tAccuracy: 0.6326704062553119\n",
      "\tMCC: 0.28849065017464165\n",
      "................................................................................\n",
      "\n",
      "Fold # 2\n",
      "\tLog loss: 0.8134058683433493\n",
      "\tAccuracy: 0.6413394526602074\n",
      "\tMCC: 0.3137299962709866\n",
      "................................................................................\n",
      "\n",
      "Fold # 3\n",
      "\tLog loss: 0.824170367969375\n",
      "\tAccuracy: 0.633968038082285\n",
      "\tMCC: 0.2945871854350786\n",
      "................................................................................\n",
      "\n",
      "Fold # 4\n",
      "\tLog loss: 0.8119837558220188\n",
      "\tAccuracy: 0.6307378442706563\n",
      "\tMCC: 0.28420430550404474\n",
      "................................................................................\n",
      "\n",
      "Fold # 5\n",
      "\tLog loss: 0.804797833813115\n",
      "\tAccuracy: 0.6441686501190071\n",
      "\tMCC: 0.32124519137839036\n",
      "................................................................................\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Min loss \t0.804797833813115\n",
      "Max loss \t0.824170367969375\n",
      "Mean loss\t0.8152966738617605\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "XGBClassifier\n",
      "\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=100, n_jobs=24,\n",
      "              num_parallel_tree=1, objective='multi:softprob', predictor='auto',\n",
      "              random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=None,\n",
      "              subsample=1, tree_method='exact', validate_parameters=1,\n",
      "              verbosity=None)\n",
      "\n",
      "Fold # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:39:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\tLog loss: 0.7651672387189233\n",
      "\tAccuracy: 0.6632670406255312\n",
      "\tMCC: 0.364647974258482\n",
      "................................................................................\n",
      "\n",
      "Fold # 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:39:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\tLog loss: 0.7636089110450549\n",
      "\tAccuracy: 0.665646778854326\n",
      "\tMCC: 0.36980306000678254\n",
      "................................................................................\n",
      "\n",
      "Fold # 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:39:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\tLog loss: 0.7630455431498148\n",
      "\tAccuracy: 0.6628697721863311\n",
      "\tMCC: 0.36315388197981685\n",
      "................................................................................\n",
      "\n",
      "Fold # 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:39:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\tLog loss: 0.7668111024286128\n",
      "\tAccuracy: 0.6524991499489969\n",
      "\tMCC: 0.34127705791147883\n",
      "................................................................................\n",
      "\n",
      "Fold # 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:40:10] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\tLog loss: 0.7425925319916824\n",
      "\tAccuracy: 0.6766405984359062\n",
      "\tMCC: 0.3940122756129144\n",
      "................................................................................\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Min loss \t0.7425925319916824\n",
      "Max loss \t0.7668111024286128\n",
      "Mean loss\t0.7602450654668177\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "LGBMClassifier\n",
      "\n",
      "LGBMClassifier()\n",
      "\n",
      "Fold # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/lightgbm/basic.py:859: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.7575762227301074\n",
      "\tAccuracy: 0.6683664796872344\n",
      "\tMCC: 0.37732358678605343\n",
      "................................................................................\n",
      "\n",
      "Fold # 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/lightgbm/basic.py:859: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.7529550437993643\n",
      "\tAccuracy: 0.6676865544790073\n",
      "\tMCC: 0.3759427085244381\n",
      "................................................................................\n",
      "\n",
      "Fold # 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/lightgbm/basic.py:859: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.7580091366374772\n",
      "\tAccuracy: 0.6652499149948997\n",
      "\tMCC: 0.3703807723027055\n",
      "................................................................................\n",
      "\n",
      "Fold # 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/lightgbm/basic.py:859: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.7587350748385786\n",
      "\tAccuracy: 0.6582794967698062\n",
      "\tMCC: 0.35566319911085575\n",
      "................................................................................\n",
      "\n",
      "Fold # 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/lightgbm/basic.py:859: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.7331417992302278\n",
      "\tAccuracy: 0.6780006800408025\n",
      "\tMCC: 0.3975987148837296\n",
      "................................................................................\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Min loss \t0.7331417992302278\n",
      "Max loss \t0.7587350748385786\n",
      "Mean loss\t0.7520834554471512\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "ExtraTreesClassifier\n",
      "\n",
      "ExtraTreesClassifier(n_jobs=-1)\n",
      "\n",
      "Fold # 1\n",
      "\tLog loss: 0.8458589326515719\n",
      "\tAccuracy: 0.6304606493285738\n",
      "\tMCC: 0.2826978634933263\n",
      "................................................................................\n",
      "\n",
      "Fold # 2\n",
      "\tLog loss: 0.807802068098926\n",
      "\tAccuracy: 0.6394696583375828\n",
      "\tMCC: 0.3083945411394423\n",
      "................................................................................\n",
      "\n",
      "Fold # 3\n",
      "\tLog loss: 0.8264758901019268\n",
      "\tAccuracy: 0.634308058483509\n",
      "\tMCC: 0.29644271301959096\n",
      "................................................................................\n",
      "\n",
      "Fold # 4\n",
      "\tLog loss: 0.8077738330209279\n",
      "\tAccuracy: 0.6315878952737164\n",
      "\tMCC: 0.28737785384684694\n",
      "................................................................................\n",
      "\n",
      "Fold # 5\n",
      "\tLog loss: 0.7995769027439527\n",
      "\tAccuracy: 0.6429785787147229\n",
      "\tMCC: 0.31877060345514413\n",
      "................................................................................\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Min loss \t0.7995769027439527\n",
      "Max loss \t0.8458589326515719\n",
      "Mean loss\t0.817497525323461\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics_dict = {}\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "text_vectorizer = TfidfVectorizer()\n",
    "standard_scaler = StandardScaler()\n",
    "\n",
    "\n",
    "for model in models:\n",
    "    model_name = type(model).__name__\n",
    "    \n",
    "    print()\n",
    "    print(80*'*')\n",
    "    print(80*'*')\n",
    "\n",
    "    print(f\"{model_name}\")\n",
    "    print()\n",
    "    print(model.__str__())\n",
    "    \n",
    "    stratified_k = StratifiedKFold(n_splits = 5, shuffle=True, random_state = RANDOM_SEED)\n",
    "    losses = []\n",
    "\n",
    "    total_mcc = 0\n",
    "    total_acc = 0\n",
    "    for idx, (train_idxs, valid_idxs) in enumerate(stratified_k.split(X_train, Y_train)):\n",
    "        print(f\"\\nFold # {idx + 1}\")\n",
    "\n",
    "        x_train, y_train = X_train.iloc[train_idxs], Y_train.iloc[train_idxs]\n",
    "        x_valid, y_valid = X_train.iloc[valid_idxs], Y_train.iloc[valid_idxs]\n",
    "\n",
    "        y_train = y_train.to_numpy()\n",
    "        y_valid = y_valid.to_numpy()\n",
    "\n",
    "        text_vectorizer.fit(x_train.essay_text.drop_duplicates())\n",
    "\n",
    "        vectorized_discourse_text_train = text_vectorizer.transform(x_train.discourse_text)\n",
    "        vectorized_discourse_text_valid = text_vectorizer.transform(x_valid.discourse_text)\n",
    "\n",
    "        ohe_discourse_type_train = ohe.fit_transform(x_train.discourse_type.values.reshape(-1, 1))\n",
    "        ohe_discourse_type_test = ohe.transform(x_valid.discourse_type.values.reshape(-1, 1))\n",
    "\n",
    "        incorrect_count_train = standard_scaler.fit_transform(x_train['incorrect_words_len'].to_numpy().reshape(-1, 1))\n",
    "        incorrect_count_valid = standard_scaler.transform(x_valid['incorrect_words_len'].to_numpy().reshape(-1, 1))\n",
    "\n",
    "        rp_start_train = x_train['rp_start'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "        rp_end_train = x_train['rp_end'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "        wps_train = x_train['words_per_sentences'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "        sps_train = x_train['syll_per_sentences'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "        spw_train = x_train['syll_per_words'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "        \n",
    "        rp_start_valid = x_valid['rp_start'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "        rp_end_valid = x_valid['rp_end'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "        wps_valid = x_valid['words_per_sentences'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "        sps_valid = x_valid['syll_per_sentences'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "        spw_valid = x_valid['syll_per_words'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "\n",
    "        x_train = sparse.hstack([vectorized_discourse_text_train, ohe_discourse_type_train, incorrect_count_train, rp_start_train, rp_end_train, wps_train, sps_train, spw_train])\n",
    "        x_valid = sparse.hstack([vectorized_discourse_text_valid, ohe_discourse_type_test, incorrect_count_valid,  rp_start_valid, rp_end_valid, wps_valid, sps_valid, spw_valid])\n",
    "        \n",
    "        if model_name == 'HistGradientBoostingClassifier':\n",
    "            x_train = x_train.todense()\n",
    "            x_valid = x_valid.todense()\n",
    "\n",
    "        model.fit(x_train, y_train)\n",
    "\n",
    "        prob_predictions = model.predict_proba(x_valid)\n",
    "        actual_pred = model.predict(x_valid)\n",
    "\n",
    "        lloss = log_loss(y_valid, prob_predictions)\n",
    "        losses.append(lloss)\n",
    "\n",
    "        acc = accuracy_score(y_valid, actual_pred)\n",
    "        mcc = matthews_corrcoef(y_valid, actual_pred)\n",
    "\n",
    "        print(f\"\\tLog loss: {lloss}\")\n",
    "        print(f\"\\tAccuracy: {acc}\")\n",
    "        print(f\"\\tMCC: {mcc}\")\n",
    "        \n",
    "        total_mcc += mcc\n",
    "        total_acc += acc\n",
    "        \n",
    "        print(80*'.')\n",
    "\n",
    "    mean_mcc = total_mcc / 5\n",
    "    mean_acc = total_acc / 5\n",
    "\n",
    "    metrics_dict[model_name] = {\n",
    "        'min_loss': min(losses),\n",
    "        'max_loss': max(losses),\n",
    "        'mean_loss': np.mean(losses),\n",
    "        'mean_acc': mean_acc, \n",
    "        'mean_mcc': mean_mcc,\n",
    "\n",
    "    } \n",
    "    \n",
    "    print()\n",
    "    print(80*'*')\n",
    "\n",
    "    print()\n",
    "\n",
    "    print(f\"Min loss \\t{min(losses)}\")\n",
    "    print(f\"Max loss \\t{max(losses)}\")\n",
    "    print(f\"Mean loss\\t{np.mean(losses)}\")\n",
    "\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-23 04:53:40.857132: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-23 04:53:40.873251: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-08-23 04:53:40.873263: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg', disable=['lemmatizer', 'tok2vec'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### This should be replaced by a pipe, for faster processing\n",
    "\n",
    "def extract_features(text):\n",
    "    doc = nlp(text)\n",
    "    tokens_ = []\n",
    "    poss_ = []\n",
    "    tags_ = []\n",
    "    morphs_ = []\n",
    "\n",
    "    for token in doc:\n",
    "        tokens_.append(token.text)\n",
    "        poss_.append(token.pos_)\n",
    "        tags_.append(token.tag_)\n",
    "        morphs_.append(token.morph)\n",
    "\n",
    "    return (tokens_, poss_, tags_, morphs_)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['discourse_features_all'] = X_train.discourse_text.map(extract_features)\n",
    "X_train['essay_features_all'] =  X_train.essay_text.map(extract_features)\n",
    "t_features = X_train.discourse_features_all.apply(pd.Series)\n",
    "t_features.columns = ['discourse_tokens', 'discourse_pos', 'discourse_tag', 'discourse_morph']\n",
    "\n",
    "e_features = X_train.essay_features_all.apply(pd.Series)\n",
    "e_features.columns = ['essay_tokens', 'essay_pos', 'essay_tag', 'essay_morph']\n",
    "\n",
    "X_train = pd.concat([X_train, t_features, e_features], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS and shallow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "LogisticRegression\n",
      "\n",
      "LogisticRegression(max_iter=1000)\n",
      "\n",
      "Fold # 1\n",
      "\tLog loss: 0.9101333378745715\n",
      "\tAccuracy: 0.588305286418494\n",
      "\tMCC: 0.15493537236203594\n",
      "................................................................................\n",
      "\n",
      "Fold # 2\n",
      "\tLog loss: 0.9039179078280468\n",
      "\tAccuracy: 0.5894951555328913\n",
      "\tMCC: 0.15816879749434298\n",
      "................................................................................\n",
      "\n",
      "Fold # 3\n",
      "\tLog loss: 0.9066648100775381\n",
      "\tAccuracy: 0.5855151309078545\n",
      "\tMCC: 0.14418550002987351\n",
      "................................................................................\n",
      "\n",
      "Fold # 4\n",
      "\tLog loss: 0.8998276693664443\n",
      "\tAccuracy: 0.5904454267256035\n",
      "\tMCC: 0.1611357361819912\n",
      "................................................................................\n",
      "\n",
      "Fold # 5\n",
      "\tLog loss: 0.8981872979545485\n",
      "\tAccuracy: 0.5894253655219314\n",
      "\tMCC: 0.1585898307311699\n",
      "................................................................................\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Min loss \t0.8981872979545485\n",
      "Max loss \t0.9101333378745715\n",
      "Mean loss\t0.9037462046202298\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "DecisionTreeClassifier\n",
      "\n",
      "DecisionTreeClassifier()\n",
      "\n",
      "Fold # 1\n",
      "\tLog loss: 16.885312510706335\n",
      "\tAccuracy: 0.5106238313785484\n",
      "\tMCC: 0.16203654567049247\n",
      "................................................................................\n",
      "\n",
      "Fold # 2\n",
      "\tLog loss: 16.65587432132613\n",
      "\tAccuracy: 0.5177630460649328\n",
      "\tMCC: 0.1774214658437609\n",
      "................................................................................\n",
      "\n",
      "Fold # 3\n",
      "\tLog loss: 16.676321822772245\n",
      "\tAccuracy: 0.5171710302618157\n",
      "\tMCC: 0.17082363861441474\n",
      "................................................................................\n",
      "\n",
      "Fold # 4\n",
      "\tLog loss: 16.870095984797416\n",
      "\tAccuracy: 0.5115606936416185\n",
      "\tMCC: 0.16253207403503675\n",
      "................................................................................\n",
      "\n",
      "Fold # 5\n",
      "\tLog loss: 16.429700162012935\n",
      "\tAccuracy: 0.5243114586875213\n",
      "\tMCC: 0.1835248811925052\n",
      "................................................................................\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Min loss \t16.429700162012935\n",
      "Max loss \t16.885312510706335\n",
      "Mean loss\t16.70346096032301\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "RandomForestClassifier\n",
      "\n",
      "RandomForestClassifier(n_jobs=-1)\n",
      "\n",
      "Fold # 1\n",
      "\tLog loss: 0.9469294201360586\n",
      "\tAccuracy: 0.60326364099949\n",
      "\tMCC: 0.24656335554137243\n",
      "................................................................................\n",
      "\n",
      "Fold # 2\n",
      "\tLog loss: 0.9679884699750503\n",
      "\tAccuracy: 0.6141424443311235\n",
      "\tMCC: 0.26858514876329437\n",
      "................................................................................\n",
      "\n",
      "Fold # 3\n",
      "\tLog loss: 0.9391228076718293\n",
      "\tAccuracy: 0.6093165589935396\n",
      "\tMCC: 0.2564177410785226\n",
      "................................................................................\n",
      "\n",
      "Fold # 4\n",
      "\tLog loss: 0.9334349034735926\n",
      "\tAccuracy: 0.6139068344100646\n",
      "\tMCC: 0.2676930369888847\n",
      "................................................................................\n",
      "\n",
      "Fold # 5\n",
      "\tLog loss: 0.9098643184522397\n",
      "\tAccuracy: 0.6302278136688201\n",
      "\tMCC: 0.30381421422466137\n",
      "................................................................................\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Min loss \t0.9098643184522397\n",
      "Max loss \t0.9679884699750503\n",
      "Mean loss\t0.939467983941754\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "XGBClassifier\n",
      "\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=100, n_jobs=24,\n",
      "              num_parallel_tree=1, objective='multi:softprob', predictor='auto',\n",
      "              random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=None,\n",
      "              subsample=1, tree_method='exact', validate_parameters=1,\n",
      "              verbosity=None)\n",
      "\n",
      "Fold # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:45:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\tLog loss: 0.8030043816835163\n",
      "\tAccuracy: 0.6442291347951725\n",
      "\tMCC: 0.32555479924067204\n",
      "................................................................................\n",
      "\n",
      "Fold # 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:45:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\tLog loss: 0.8020393522056166\n",
      "\tAccuracy: 0.6477987421383647\n",
      "\tMCC: 0.333413352427533\n",
      "................................................................................\n",
      "\n",
      "Fold # 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:45:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\tLog loss: 0.8036751204801605\n",
      "\tAccuracy: 0.6445086705202312\n",
      "\tMCC: 0.3247963435167674\n",
      "................................................................................\n",
      "\n",
      "Fold # 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:45:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\tLog loss: 0.8042295533406955\n",
      "\tAccuracy: 0.6417885073104386\n",
      "\tMCC: 0.3192972059039802\n",
      "................................................................................\n",
      "\n",
      "Fold # 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:45:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\tLog loss: 0.7635146597786169\n",
      "\tAccuracy: 0.6640598435906154\n",
      "\tMCC: 0.36858025835773917\n",
      "................................................................................\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Min loss \t0.7635146597786169\n",
      "Max loss \t0.8042295533406955\n",
      "Mean loss\t0.7952926134977212\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "LGBMClassifier\n",
      "\n",
      "LGBMClassifier()\n",
      "\n",
      "Fold # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/lightgbm/basic.py:859: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.7938285454655596\n",
      "\tAccuracy: 0.6505184429712731\n",
      "\tMCC: 0.335480730100658\n",
      "................................................................................\n",
      "\n",
      "Fold # 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/lightgbm/basic.py:859: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.79568594555847\n",
      "\tAccuracy: 0.6537480877103519\n",
      "\tMCC: 0.3420156680336404\n",
      "................................................................................\n",
      "\n",
      "Fold # 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/lightgbm/basic.py:859: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.7930764940912804\n",
      "\tAccuracy: 0.6506290377422645\n",
      "\tMCC: 0.3343761742235196\n",
      "................................................................................\n",
      "\n",
      "Fold # 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/lightgbm/basic.py:859: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.7989706657272928\n",
      "\tAccuracy: 0.6431485889153349\n",
      "\tMCC: 0.31845219801871444\n",
      "................................................................................\n",
      "\n",
      "Fold # 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/lightgbm/basic.py:859: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.7649250616062437\n",
      "\tAccuracy: 0.6696701802108126\n",
      "\tMCC: 0.3779438711047319\n",
      "................................................................................\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Min loss \t0.7649250616062437\n",
      "Max loss \t0.7989706657272928\n",
      "Mean loss\t0.7892973424897693\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "ExtraTreesClassifier\n",
      "\n",
      "ExtraTreesClassifier(n_jobs=-1)\n",
      "\n",
      "Fold # 1\n",
      "\tLog loss: 1.1351061811595624\n",
      "\tAccuracy: 0.5908550059493456\n",
      "\tMCC: 0.23287827086214702\n",
      "................................................................................\n",
      "\n",
      "Fold # 2\n",
      "\tLog loss: 1.2130840598216484\n",
      "\tAccuracy: 0.59799422063573\n",
      "\tMCC: 0.24920711346594696\n",
      "................................................................................\n",
      "\n",
      "Fold # 3\n",
      "\tLog loss: 1.130443113558469\n",
      "\tAccuracy: 0.5921455287317239\n",
      "\tMCC: 0.23479561442059757\n",
      "................................................................................\n",
      "\n",
      "Fold # 4\n",
      "\tLog loss: 1.13065536582125\n",
      "\tAccuracy: 0.6001360081604896\n",
      "\tMCC: 0.24945257118672493\n",
      "................................................................................\n",
      "\n",
      "Fold # 5\n",
      "\tLog loss: 1.0830011040786778\n",
      "\tAccuracy: 0.6105066303978238\n",
      "\tMCC: 0.27792413761571194\n",
      "................................................................................\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Min loss \t1.0830011040786778\n",
      "Max loss \t1.2130840598216484\n",
      "Mean loss\t1.1384579648879214\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "metrics_dict_2 = {}\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "\n",
    "text_vectorizer = TfidfVectorizer()\n",
    "pos_vectorizer = TfidfVectorizer(ngram_range=(1,4))\n",
    "standard_scaler = StandardScaler()\n",
    "\n",
    "\n",
    "for model in models:\n",
    "    model_name = type(model).__name__\n",
    "    \n",
    "    print()\n",
    "    print(80*'*')\n",
    "    print(80*'*')\n",
    "\n",
    "    print(f\"{model_name}\")\n",
    "    print()\n",
    "    print(model.__str__())\n",
    "    \n",
    "    stratified_k = StratifiedKFold(n_splits = 5, shuffle=True, random_state = RANDOM_SEED)\n",
    "    losses = []\n",
    "\n",
    "    total_mcc = 0\n",
    "    total_acc = 0\n",
    "    \n",
    "    for idx, (train_idxs, valid_idxs) in enumerate(stratified_k.split(X_train, Y_train)):\n",
    "        print(f\"\\nFold # {idx + 1}\")\n",
    "\n",
    "        x_train, y_train = X_train.iloc[train_idxs], Y_train.iloc[train_idxs]\n",
    "        x_valid, y_valid = X_train.iloc[valid_idxs], Y_train.iloc[valid_idxs]\n",
    "\n",
    "        y_train = y_train.to_numpy()\n",
    "        y_valid = y_valid.to_numpy()\n",
    "\n",
    "        # text_vectorizer.fit(x_train.essay_text.drop_duplicates())\n",
    "\n",
    "        # vectorized_discourse_text_train = text_vectorizer.transform(x_train.discourse_text)\n",
    "        # vectorized_discourse_text_valid = text_vectorizer.transform(x_valid.discourse_text)\n",
    "\n",
    "        ohe_discourse_type_train = ohe.fit_transform(x_train.discourse_type.values.reshape(-1, 1))\n",
    "        ohe_discourse_type_test = ohe.transform(x_valid.discourse_type.values.reshape(-1, 1))\n",
    "\n",
    "        incorrect_count_train = standard_scaler.fit_transform(x_train['incorrect_words_len'].to_numpy().reshape(-1, 1))\n",
    "        incorrect_count_valid = standard_scaler.transform(x_valid['incorrect_words_len'].to_numpy().reshape(-1, 1))\n",
    "\n",
    "        rp_start_train = x_train['rp_start'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "        rp_end_train = x_train['rp_end'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "        wps_train = x_train['words_per_sentences'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "        sps_train = x_train['syll_per_sentences'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "        spw_train = x_train['syll_per_words'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "        \n",
    "        rp_start_valid = x_valid['rp_start'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "        rp_end_valid = x_valid['rp_end'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "        wps_valid = x_valid['words_per_sentences'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "        sps_valid = x_valid['syll_per_sentences'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "        spw_valid = x_valid['syll_per_words'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "\n",
    "        pos_features_train = pos_vectorizer.fit_transform(x_train.discourse_pos.map(' '.join))\n",
    "        pos_features_valid = pos_vectorizer.transform(x_valid.discourse_pos.map(' '.join))\n",
    "\n",
    "        x_train = sparse.hstack([pos_features_train, ohe_discourse_type_train, incorrect_count_train, rp_start_train, rp_end_train, wps_train, sps_train, spw_train])\n",
    "        x_valid = sparse.hstack([pos_features_valid, ohe_discourse_type_test, incorrect_count_valid,  rp_start_valid, rp_end_valid, wps_valid, sps_valid, spw_valid])\n",
    "        \n",
    "\n",
    "        model.fit(x_train, y_train)\n",
    "\n",
    "        prob_predictions = model.predict_proba(x_valid)\n",
    "        actual_pred = model.predict(x_valid)\n",
    "\n",
    "        lloss = log_loss(y_valid, prob_predictions)\n",
    "        losses.append(lloss)\n",
    "\n",
    "        acc = accuracy_score(y_valid, actual_pred)\n",
    "        mcc = matthews_corrcoef(y_valid, actual_pred)\n",
    "\n",
    "        print(f\"\\tLog loss: {lloss}\")\n",
    "        print(f\"\\tAccuracy: {acc}\")\n",
    "        print(f\"\\tMCC: {mcc}\")\n",
    "        \n",
    "        total_mcc += mcc\n",
    "        total_acc += acc\n",
    "        \n",
    "        print(80*'.')\n",
    "\n",
    "    mean_mcc = total_mcc / 5\n",
    "    mean_acc = total_acc / 5\n",
    "\n",
    "    metrics_dict_2[model_name] = {\n",
    "        'min_loss': min(losses),\n",
    "        'max_loss': max(losses),\n",
    "        'mean_loss': np.mean(losses),\n",
    "        'mean_acc': mean_acc, \n",
    "        'mean_mcc': mean_mcc,\n",
    "\n",
    "    } \n",
    "    \n",
    "    print()\n",
    "    print(80*'*')\n",
    "\n",
    "    print()\n",
    "\n",
    "    print(f\"Min loss \\t{min(losses)}\")\n",
    "    print(f\"Max loss \\t{max(losses)}\")\n",
    "    print(f\"Mean loss\\t{np.mean(losses)}\")\n",
    "\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text-features, POS and shallow.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "LogisticRegression\n",
      "\n",
      "LogisticRegression(max_iter=1000)\n",
      "\n",
      "Fold # 1\n",
      "\tLog loss: 0.7705963692005907\n",
      "\tAccuracy: 0.65595784463709\n",
      "\tMCC: 0.35665416016647433\n",
      "................................................................................\n",
      "\n",
      "Fold # 2\n",
      "\tLog loss: 0.7636174660116537\n",
      "\tAccuracy: 0.6579976202617712\n",
      "\tMCC: 0.36028031492572027\n",
      "................................................................................\n",
      "\n",
      "Fold # 3\n",
      "\tLog loss: 0.7721678256656753\n",
      "\tAccuracy: 0.6540292417545053\n",
      "\tMCC: 0.3517270497635027\n",
      "................................................................................\n",
      "\n",
      "Fold # 4\n",
      "\tLog loss: 0.7633721651470433\n",
      "\tAccuracy: 0.6584495069704183\n",
      "\tMCC: 0.35914826043801795\n",
      "................................................................................\n",
      "\n",
      "Fold # 5\n",
      "\tLog loss: 0.7468061674669159\n",
      "\tAccuracy: 0.6720503230193812\n",
      "\tMCC: 0.38849550027842383\n",
      "................................................................................\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Min loss \t0.7468061674669159\n",
      "Max loss \t0.7721678256656753\n",
      "Mean loss\t0.7633119986983758\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "DecisionTreeClassifier\n",
      "\n",
      "DecisionTreeClassifier()\n",
      "\n",
      "Fold # 1\n",
      "\tLog loss: 15.816329017149306\n",
      "\tAccuracy: 0.5420703722590515\n",
      "\tMCC: 0.19707908088180617\n",
      "................................................................................\n",
      "\n",
      "Fold # 2\n",
      "\tLog loss: 15.904393209895128\n",
      "\tAccuracy: 0.5395206527281999\n",
      "\tMCC: 0.18672542591347302\n",
      "................................................................................\n",
      "\n",
      "Fold # 3\n",
      "\tLog loss: 15.801402121507081\n",
      "\tAccuracy: 0.5425025501530092\n",
      "\tMCC: 0.19308240925517667\n",
      "................................................................................\n",
      "\n",
      "Fold # 4\n",
      "\tLog loss: 15.89547107245899\n",
      "\tAccuracy: 0.5396123767426045\n",
      "\tMCC: 0.18741602997341\n",
      "................................................................................\n",
      "\n",
      "Fold # 5\n",
      "\tLog loss: 15.81901795441846\n",
      "\tAccuracy: 0.5419925195511731\n",
      "\tMCC: 0.19712564903334023\n",
      "................................................................................\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Min loss \t15.801402121507081\n",
      "Max loss \t15.904393209895128\n",
      "Mean loss\t15.847322675085792\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "RandomForestClassifier\n",
      "\n",
      "RandomForestClassifier(n_jobs=-1)\n",
      "\n",
      "Fold # 1\n",
      "\tLog loss: 0.8203990348552368\n",
      "\tAccuracy: 0.6384497705252422\n",
      "\tMCC: 0.30279385809578147\n",
      "................................................................................\n",
      "\n",
      "Fold # 2\n",
      "\tLog loss: 0.8123591879677844\n",
      "\tAccuracy: 0.643889172191059\n",
      "\tMCC: 0.3182044529643575\n",
      "................................................................................\n",
      "\n",
      "Fold # 3\n",
      "\tLog loss: 0.8110782850724093\n",
      "\tAccuracy: 0.6397483849030942\n",
      "\tMCC: 0.3075769448597777\n",
      "................................................................................\n",
      "\n",
      "Fold # 4\n",
      "\tLog loss: 0.8114248746776684\n",
      "\tAccuracy: 0.634138048282897\n",
      "\tMCC: 0.29174415385898883\n",
      "................................................................................\n",
      "\n",
      "Fold # 5\n",
      "\tLog loss: 0.8100954494527491\n",
      "\tAccuracy: 0.6451887113226794\n",
      "\tMCC: 0.32186297842533096\n",
      "................................................................................\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Min loss \t0.8100954494527491\n",
      "Max loss \t0.8203990348552368\n",
      "Mean loss\t0.8130713664051697\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "XGBClassifier\n",
      "\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=100, n_jobs=24,\n",
      "              num_parallel_tree=1, objective='multi:softprob', predictor='auto',\n",
      "              random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=None,\n",
      "              subsample=1, tree_method='exact', validate_parameters=1,\n",
      "              verbosity=None)\n",
      "\n",
      "Fold # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:47:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\tLog loss: 0.7597860706197634\n",
      "\tAccuracy: 0.6613972463029066\n",
      "\tMCC: 0.36079040565462545\n",
      "................................................................................\n",
      "\n",
      "Fold # 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:48:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\tLog loss: 0.7577922335834093\n",
      "\tAccuracy: 0.6624171341152473\n",
      "\tMCC: 0.3643572994472992\n",
      "................................................................................\n",
      "\n",
      "Fold # 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:48:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\tLog loss: 0.7617651269594536\n",
      "\tAccuracy: 0.6608296497789867\n",
      "\tMCC: 0.35921050700623275\n",
      "................................................................................\n",
      "\n",
      "Fold # 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:48:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\tLog loss: 0.7584690873914334\n",
      "\tAccuracy: 0.6613396803808228\n",
      "\tMCC: 0.36085557756036957\n",
      "................................................................................\n",
      "\n",
      "Fold # 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:48:31] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\tLog loss: 0.7377060311779143\n",
      "\tAccuracy: 0.6764705882352942\n",
      "\tMCC: 0.3938447383885428\n",
      "................................................................................\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Min loss \t0.7377060311779143\n",
      "Max loss \t0.7617651269594536\n",
      "Mean loss\t0.7551037099463949\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "LGBMClassifier\n",
      "\n",
      "LGBMClassifier()\n",
      "\n",
      "Fold # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/lightgbm/basic.py:859: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.7480765381457647\n",
      "\tAccuracy: 0.6710861805201428\n",
      "\tMCC: 0.38323858997953253\n",
      "................................................................................\n",
      "\n",
      "Fold # 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/lightgbm/basic.py:859: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.7464304527094678\n",
      "\tAccuracy: 0.6690464048954615\n",
      "\tMCC: 0.37897039229049634\n",
      "................................................................................\n",
      "\n",
      "Fold # 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/lightgbm/basic.py:859: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.7520788679666738\n",
      "\tAccuracy: 0.6642298537912275\n",
      "\tMCC: 0.36796321524405856\n",
      "................................................................................\n",
      "\n",
      "Fold # 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/lightgbm/basic.py:859: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.7472275531429659\n",
      "\tAccuracy: 0.6660999659979598\n",
      "\tMCC: 0.37256274388486027\n",
      "................................................................................\n",
      "\n",
      "Fold # 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/lightgbm/basic.py:859: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.7244428339683451\n",
      "\tAccuracy: 0.6827609656579394\n",
      "\tMCC: 0.4076017648952945\n",
      "................................................................................\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Min loss \t0.7244428339683451\n",
      "Max loss \t0.7520788679666738\n",
      "Mean loss\t0.7436512491866434\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "ExtraTreesClassifier\n",
      "\n",
      "ExtraTreesClassifier(n_jobs=-1)\n",
      "\n",
      "Fold # 1\n",
      "\tLog loss: 0.8253641548034651\n",
      "\tAccuracy: 0.6350501444841068\n",
      "\tMCC: 0.2947747407261423\n",
      "................................................................................\n",
      "\n",
      "Fold # 2\n",
      "\tLog loss: 0.8105449935896141\n",
      "\tAccuracy: 0.6450790413054563\n",
      "\tMCC: 0.32474878754390457\n",
      "................................................................................\n",
      "\n",
      "Fold # 3\n",
      "\tLog loss: 0.8119073776167025\n",
      "\tAccuracy: 0.6363481808908534\n",
      "\tMCC: 0.3010327009224727\n",
      "................................................................................\n",
      "\n",
      "Fold # 4\n",
      "\tLog loss: 0.8047947912394996\n",
      "\tAccuracy: 0.6310778646718803\n",
      "\tMCC: 0.2860966836227921\n",
      "................................................................................\n",
      "\n",
      "Fold # 5\n",
      "\tLog loss: 0.8062547599407379\n",
      "\tAccuracy: 0.648248894933696\n",
      "\tMCC: 0.33268056114962247\n",
      "................................................................................\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Min loss \t0.8047947912394996\n",
      "Max loss \t0.8253641548034651\n",
      "Mean loss\t0.811773215438004\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pos_vectorizer = TfidfVectorizer(ngram_range=(1,4))\n",
    "metrics_dict_3 = {}\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "text_vectorizer = TfidfVectorizer()\n",
    "pos_vectorizer = TfidfVectorizer(ngram_range=(1,4))\n",
    "standard_scaler = StandardScaler()\n",
    "\n",
    "\n",
    "for model in models:\n",
    "    model_name = type(model).__name__\n",
    "    \n",
    "    print()\n",
    "    print(80*'*')\n",
    "    print(80*'*')\n",
    "\n",
    "    print(f\"{model_name}\")\n",
    "    print()\n",
    "    print(model.__str__())\n",
    "    \n",
    "    stratified_k = StratifiedKFold(n_splits = 5, shuffle=True, random_state = RANDOM_SEED)\n",
    "    losses = []\n",
    "\n",
    "    total_mcc = 0\n",
    "    total_acc = 0\n",
    "    for idx, (train_idxs, valid_idxs) in enumerate(stratified_k.split(X_train, Y_train)):\n",
    "        print(f\"\\nFold # {idx + 1}\")\n",
    "\n",
    "        x_train, y_train = X_train.iloc[train_idxs], Y_train.iloc[train_idxs]\n",
    "        x_valid, y_valid = X_train.iloc[valid_idxs], Y_train.iloc[valid_idxs]\n",
    "\n",
    "        y_train = y_train.to_numpy()\n",
    "        y_valid = y_valid.to_numpy()\n",
    "\n",
    "        text_vectorizer.fit(x_train.essay_text.drop_duplicates())\n",
    "\n",
    "        vectorized_discourse_text_train = text_vectorizer.transform(x_train.discourse_text)\n",
    "        vectorized_discourse_text_valid = text_vectorizer.transform(x_valid.discourse_text)\n",
    "\n",
    "        ohe_discourse_type_train = ohe.fit_transform(x_train.discourse_type.values.reshape(-1, 1))\n",
    "        ohe_discourse_type_test = ohe.transform(x_valid.discourse_type.values.reshape(-1, 1))\n",
    "\n",
    "        incorrect_count_train = standard_scaler.fit_transform(x_train['incorrect_words_len'].to_numpy().reshape(-1, 1))\n",
    "        incorrect_count_valid = standard_scaler.transform(x_valid['incorrect_words_len'].to_numpy().reshape(-1, 1))\n",
    "\n",
    "        rp_start_train = x_train['rp_start'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "        rp_end_train = x_train['rp_end'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "        wps_train = x_train['words_per_sentences'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "        sps_train = x_train['syll_per_sentences'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "        spw_train = x_train['syll_per_words'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "        \n",
    "        rp_start_valid = x_valid['rp_start'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "        rp_end_valid = x_valid['rp_end'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "        wps_valid = x_valid['words_per_sentences'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "        sps_valid = x_valid['syll_per_sentences'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "        spw_valid = x_valid['syll_per_words'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "\n",
    "        pos_features_train = pos_vectorizer.fit_transform(x_train.discourse_pos.map(' '.join))\n",
    "        pos_features_valid = pos_vectorizer.transform(x_valid.discourse_pos.map(' '.join))\n",
    "\n",
    "        x_train = sparse.hstack([vectorized_discourse_text_train, ohe_discourse_type_train, pos_features_train, incorrect_count_train, rp_start_train, rp_end_train, wps_train, sps_train, spw_train])\n",
    "        x_valid = sparse.hstack([vectorized_discourse_text_valid, ohe_discourse_type_test, pos_features_valid, incorrect_count_valid,  rp_start_valid, rp_end_valid, wps_valid, sps_valid, spw_valid])\n",
    "        \n",
    "\n",
    "        model.fit(x_train, y_train)\n",
    "\n",
    "        prob_predictions = model.predict_proba(x_valid)\n",
    "        actual_pred = model.predict(x_valid)\n",
    "\n",
    "        lloss = log_loss(y_valid, prob_predictions)\n",
    "        losses.append(lloss)\n",
    "\n",
    "        acc = accuracy_score(y_valid, actual_pred)\n",
    "        mcc = matthews_corrcoef(y_valid, actual_pred)\n",
    "\n",
    "        print(f\"\\tLog loss: {lloss}\")\n",
    "        print(f\"\\tAccuracy: {acc}\")\n",
    "        print(f\"\\tMCC: {mcc}\")\n",
    "        \n",
    "        total_mcc += mcc\n",
    "        total_acc += acc\n",
    "        \n",
    "        print(80*'.')\n",
    "\n",
    "    mean_mcc = total_mcc / 5\n",
    "    mean_acc = total_acc / 5\n",
    "\n",
    "    metrics_dict_3[model_name] = {\n",
    "        'min_loss': min(losses),\n",
    "        'max_loss': max(losses),\n",
    "        'mean_loss': np.mean(losses),\n",
    "        'mean_acc': mean_acc, \n",
    "        'mean_mcc': mean_mcc,\n",
    "\n",
    "    } \n",
    "    \n",
    "    print()\n",
    "    print(80*'*')\n",
    "\n",
    "    print()\n",
    "\n",
    "    print(f\"Min loss \\t{min(losses)}\")\n",
    "    print(f\"Max loss \\t{max(losses)}\")\n",
    "    print(f\"Mean loss\\t{np.mean(losses)}\")\n",
    "\n",
    "    print()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['contains_question'] = X_train.discourse_text.str.contains('\\?').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "LogisticRegression\n",
      "\n",
      "LogisticRegression(max_iter=1000)\n",
      "\n",
      "Fold # 1\n",
      "\tLog loss: 0.7703378799719015\n",
      "\tAccuracy: 0.6540880503144654\n",
      "\tMCC: 0.35304907836407257\n",
      "................................................................................\n",
      "\n",
      "Fold # 2\n",
      "\tLog loss: 0.7633629428366933\n",
      "\tAccuracy: 0.6562978072412035\n",
      "\tMCC: 0.3568880725703735\n",
      "................................................................................\n",
      "\n",
      "Fold # 3\n",
      "\tLog loss: 0.7716869968822638\n",
      "\tAccuracy: 0.6545392723563414\n",
      "\tMCC: 0.3527759213405058\n",
      "................................................................................\n",
      "\n",
      "Fold # 4\n",
      "\tLog loss: 0.7633300080192839\n",
      "\tAccuracy: 0.657089425365522\n",
      "\tMCC: 0.3565296453431742\n",
      "................................................................................\n",
      "\n",
      "Fold # 5\n",
      "\tLog loss: 0.7472652309513227\n",
      "\tAccuracy: 0.6722203332199932\n",
      "\tMCC: 0.3890953668139942\n",
      "................................................................................\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Min loss \t0.7472652309513227\n",
      "Max loss \t0.7716869968822638\n",
      "Mean loss\t0.7631966117322929\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "DecisionTreeClassifier\n",
      "\n",
      "DecisionTreeClassifier()\n",
      "\n",
      "Fold # 1\n",
      "\tLog loss: 15.939736709053724\n",
      "\tAccuracy: 0.5385007649158593\n",
      "\tMCC: 0.19231120152999204\n",
      "................................................................................\n",
      "\n",
      "Fold # 2\n",
      "\tLog loss: 15.90439320989513\n",
      "\tAccuracy: 0.5395206527281999\n",
      "\tMCC: 0.1887536643714881\n",
      "................................................................................\n",
      "\n",
      "Fold # 3\n",
      "\tLog loss: 15.8015199635983\n",
      "\tAccuracy: 0.5423325399523972\n",
      "\tMCC: 0.19270543325997944\n",
      "................................................................................\n",
      "\n",
      "Fold # 4\n",
      "\tLog loss: 15.713440799041408\n",
      "\tAccuracy: 0.5448826929615777\n",
      "\tMCC: 0.19610501961927498\n",
      "................................................................................\n",
      "\n",
      "Fold # 5\n",
      "\tLog loss: 15.865993508848803\n",
      "\tAccuracy: 0.5406324379462768\n",
      "\tMCC: 0.1987469325120087\n",
      "................................................................................\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Min loss \t15.713440799041408\n",
      "Max loss \t15.939736709053724\n",
      "Mean loss\t15.845016838087474\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "RandomForestClassifier\n",
      "\n",
      "RandomForestClassifier(n_jobs=-1)\n",
      "\n",
      "Fold # 1\n",
      "\tLog loss: 0.8207007201003712\n",
      "\tAccuracy: 0.6398096209416965\n",
      "\tMCC: 0.305777458247449\n",
      "................................................................................\n",
      "\n",
      "Fold # 2\n",
      "\tLog loss: 0.8126365937209523\n",
      "\tAccuracy: 0.6460989291177971\n",
      "\tMCC: 0.32359461365968034\n",
      "................................................................................\n",
      "\n",
      "Fold # 3\n",
      "\tLog loss: 0.8136213000516435\n",
      "\tAccuracy: 0.6431485889153349\n",
      "\tMCC: 0.31698522903485454\n",
      "................................................................................\n",
      "\n",
      "Fold # 4\n",
      "\tLog loss: 0.8139224701058057\n",
      "\tAccuracy: 0.6370282216933016\n",
      "\tMCC: 0.30015289017371677\n",
      "................................................................................\n",
      "\n",
      "Fold # 5\n",
      "\tLog loss: 0.8056989356267727\n",
      "\tAccuracy: 0.6473988439306358\n",
      "\tMCC: 0.3280933060786012\n",
      "................................................................................\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Min loss \t0.8056989356267727\n",
      "Max loss \t0.8207007201003712\n",
      "Mean loss\t0.8133160039211091\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "XGBClassifier\n",
      "\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=100, n_jobs=24,\n",
      "              num_parallel_tree=1, objective='multi:softprob', predictor='auto',\n",
      "              random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=None,\n",
      "              subsample=1, tree_method='exact', validate_parameters=1,\n",
      "              verbosity=None)\n",
      "\n",
      "Fold # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:51:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\tLog loss: 0.7588666997212438\n",
      "\tAccuracy: 0.6622471528131906\n",
      "\tMCC: 0.36290241863720907\n",
      "................................................................................\n",
      "\n",
      "Fold # 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:51:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\tLog loss: 0.7578434216565579\n",
      "\tAccuracy: 0.6666666666666666\n",
      "\tMCC: 0.372973505499524\n",
      "................................................................................\n",
      "\n",
      "Fold # 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:51:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\tLog loss: 0.7612158324536926\n",
      "\tAccuracy: 0.661849710982659\n",
      "\tMCC: 0.36137251241922014\n",
      "................................................................................\n",
      "\n",
      "Fold # 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:52:06] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\tLog loss: 0.7598843677089347\n",
      "\tAccuracy: 0.662019721183271\n",
      "\tMCC: 0.3624519983889905\n",
      "................................................................................\n",
      "\n",
      "Fold # 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:52:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\tLog loss: 0.7370382852622601\n",
      "\tAccuracy: 0.6780006800408025\n",
      "\tMCC: 0.3970556163056401\n",
      "................................................................................\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Min loss \t0.7370382852622601\n",
      "Max loss \t0.7612158324536926\n",
      "Mean loss\t0.7549697213605379\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "LGBMClassifier\n",
      "\n",
      "LGBMClassifier()\n",
      "\n",
      "Fold # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/lightgbm/basic.py:859: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.7463377983595622\n",
      "\tAccuracy: 0.6707462179160293\n",
      "\tMCC: 0.38249454356819035\n",
      "................................................................................\n",
      "\n",
      "Fold # 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/lightgbm/basic.py:859: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.7458324841580299\n",
      "\tAccuracy: 0.667176610572837\n",
      "\tMCC: 0.37495120284266054\n",
      "................................................................................\n",
      "\n",
      "Fold # 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/lightgbm/basic.py:859: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.751389072621329\n",
      "\tAccuracy: 0.6678000680040802\n",
      "\tMCC: 0.3755633444332696\n",
      "................................................................................\n",
      "\n",
      "Fold # 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/lightgbm/basic.py:859: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.7483308170082826\n",
      "\tAccuracy: 0.6630397823869432\n",
      "\tMCC: 0.3664218172744422\n",
      "................................................................................\n",
      "\n",
      "Fold # 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/lightgbm/basic.py:859: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.7225697317441382\n",
      "\tAccuracy: 0.6863311798707923\n",
      "\tMCC: 0.41517582225676947\n",
      "................................................................................\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Min loss \t0.7225697317441382\n",
      "Max loss \t0.751389072621329\n",
      "Mean loss\t0.7428919807782683\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "ExtraTreesClassifier\n",
      "\n",
      "ExtraTreesClassifier(n_jobs=-1)\n",
      "\n",
      "Fold # 1\n",
      "\tLog loss: 0.8145978120297883\n",
      "\tAccuracy: 0.6350501444841068\n",
      "\tMCC: 0.29513577473581926\n",
      "................................................................................\n",
      "\n",
      "Fold # 2\n",
      "\tLog loss: 0.8109692759728797\n",
      "\tAccuracy: 0.6449090600033996\n",
      "\tMCC: 0.32425837727235957\n",
      "................................................................................\n",
      "\n",
      "Fold # 3\n",
      "\tLog loss: 0.8116481749067733\n",
      "\tAccuracy: 0.6371982318939137\n",
      "\tMCC: 0.30347702886802147\n",
      "................................................................................\n",
      "\n",
      "Fold # 4\n",
      "\tLog loss: 0.8044192804091473\n",
      "\tAccuracy: 0.634478068684121\n",
      "\tMCC: 0.2947373476221486\n",
      "................................................................................\n",
      "\n",
      "Fold # 5\n",
      "\tLog loss: 0.8053689205546634\n",
      "\tAccuracy: 0.6477388643318599\n",
      "\tMCC: 0.3317303799805145\n",
      "................................................................................\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Min loss \t0.8044192804091473\n",
      "Max loss \t0.8145978120297883\n",
      "Mean loss\t0.8094006927746504\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pos_vectorizer = TfidfVectorizer(ngram_range=(1,4))\n",
    "metrics_dict_4 = {}\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "text_vectorizer = TfidfVectorizer()\n",
    "pos_vectorizer = TfidfVectorizer(ngram_range=(1,4))\n",
    "standard_scaler = StandardScaler()\n",
    "\n",
    "\n",
    "for model in models:\n",
    "    model_name = type(model).__name__\n",
    "    \n",
    "    print()\n",
    "    print(80*'*')\n",
    "    print(80*'*')\n",
    "\n",
    "    print(f\"{model_name}\")\n",
    "    print()\n",
    "    print(model.__str__())\n",
    "    \n",
    "    stratified_k = StratifiedKFold(n_splits = 5, shuffle=True, random_state = RANDOM_SEED)\n",
    "    losses = []\n",
    "\n",
    "    total_mcc = 0\n",
    "    total_acc = 0\n",
    "    for idx, (train_idxs, valid_idxs) in enumerate(stratified_k.split(X_train, Y_train)):\n",
    "        print(f\"\\nFold # {idx + 1}\")\n",
    "\n",
    "        x_train, y_train = X_train.iloc[train_idxs], Y_train.iloc[train_idxs]\n",
    "        x_valid, y_valid = X_train.iloc[valid_idxs], Y_train.iloc[valid_idxs]\n",
    "\n",
    "        y_train = y_train.to_numpy()\n",
    "        y_valid = y_valid.to_numpy()\n",
    "\n",
    "        text_vectorizer.fit(x_train.essay_text.drop_duplicates())\n",
    "\n",
    "        vectorized_discourse_text_train = text_vectorizer.transform(x_train.discourse_text)\n",
    "        vectorized_discourse_text_valid = text_vectorizer.transform(x_valid.discourse_text)\n",
    "\n",
    "        ohe_discourse_type_train = ohe.fit_transform(x_train.discourse_type.values.reshape(-1, 1))\n",
    "        ohe_discourse_type_test = ohe.transform(x_valid.discourse_type.values.reshape(-1, 1))\n",
    "\n",
    "        incorrect_count_train = standard_scaler.fit_transform(x_train['incorrect_words_len'].to_numpy().reshape(-1, 1))\n",
    "        incorrect_count_valid = standard_scaler.transform(x_valid['incorrect_words_len'].to_numpy().reshape(-1, 1))\n",
    "\n",
    "        rp_start_train = x_train['rp_start'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "        rp_end_train = x_train['rp_end'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "        wps_train = x_train['words_per_sentences'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "        sps_train = x_train['syll_per_sentences'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "        spw_train = x_train['syll_per_words'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "        cqt_train = x_train['contains_question'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "\n",
    "        rp_start_valid = x_valid['rp_start'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "        rp_end_valid = x_valid['rp_end'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "        wps_valid = x_valid['words_per_sentences'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "        sps_valid = x_valid['syll_per_sentences'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "        spw_valid = x_valid['syll_per_words'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "        cqt_valid = x_valid['contains_question'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "\n",
    "        pos_features_train = pos_vectorizer.fit_transform(x_train.discourse_pos.map(' '.join))\n",
    "        pos_features_valid = pos_vectorizer.transform(x_valid.discourse_pos.map(' '.join))\n",
    "\n",
    "        x_train = sparse.hstack([vectorized_discourse_text_train, ohe_discourse_type_train, pos_features_train, incorrect_count_train, rp_start_train, rp_end_train, wps_train, sps_train, spw_train,cqt_train])\n",
    "        x_valid = sparse.hstack([vectorized_discourse_text_valid, ohe_discourse_type_test, pos_features_valid, incorrect_count_valid,  rp_start_valid, rp_end_valid, wps_valid, sps_valid, spw_valid, cqt_valid])\n",
    "        \n",
    "\n",
    "        model.fit(x_train, y_train)\n",
    "\n",
    "        prob_predictions = model.predict_proba(x_valid)\n",
    "        actual_pred = model.predict(x_valid)\n",
    "\n",
    "        lloss = log_loss(y_valid, prob_predictions)\n",
    "        losses.append(lloss)\n",
    "\n",
    "        acc = accuracy_score(y_valid, actual_pred)\n",
    "        mcc = matthews_corrcoef(y_valid, actual_pred)\n",
    "\n",
    "        print(f\"\\tLog loss: {lloss}\")\n",
    "        print(f\"\\tAccuracy: {acc}\")\n",
    "        print(f\"\\tMCC: {mcc}\")\n",
    "        \n",
    "        total_mcc += mcc\n",
    "        total_acc += acc\n",
    "        \n",
    "        print(80*'.')\n",
    "\n",
    "    mean_mcc = total_mcc / 5\n",
    "    mean_acc = total_acc / 5\n",
    "\n",
    "    metrics_dict_4[model_name] = {\n",
    "        'min_loss': min(losses),\n",
    "        'max_loss': max(losses),\n",
    "        'mean_loss': np.mean(losses),\n",
    "        'mean_acc': mean_acc, \n",
    "        'mean_mcc': mean_mcc,\n",
    "\n",
    "    } \n",
    "    \n",
    "    print()\n",
    "    print(80*'*')\n",
    "\n",
    "    print()\n",
    "\n",
    "    print(f\"Min loss \\t{min(losses)}\")\n",
    "    print(f\"Max loss \\t{max(losses)}\")\n",
    "    print(f\"Mean loss\\t{np.mean(losses)}\")\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "LogisticRegression\n",
      "\n",
      "LogisticRegression(max_iter=1000)\n",
      "\n",
      "Fold # 1\n",
      "\tLog loss: 0.769322096239571\n",
      "\tAccuracy: 0.6605473397926228\n",
      "\tMCC: 0.3591225364127667\n",
      "................................................................................\n",
      "\n",
      "Fold # 2\n",
      "\tLog loss: 0.7583666222877341\n",
      "\tAccuracy: 0.6668366479687234\n",
      "\tMCC: 0.3728724282481718\n",
      "................................................................................\n",
      "\n",
      "Fold # 3\n",
      "\tLog loss: 0.7659107129855988\n",
      "\tAccuracy: 0.6611696701802108\n",
      "\tMCC: 0.3592661340796329\n",
      "................................................................................\n",
      "\n",
      "Fold # 4\n",
      "\tLog loss: 0.7633638855592403\n",
      "\tAccuracy: 0.661849710982659\n",
      "\tMCC: 0.36115421525829017\n",
      "................................................................................\n",
      "\n",
      "Fold # 5\n",
      "\tLog loss: 0.7461715144799874\n",
      "\tAccuracy: 0.6780006800408025\n",
      "\tMCC: 0.3971587237150263\n",
      "................................................................................\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Min loss \t0.7461715144799874\n",
      "Max loss \t0.769322096239571\n",
      "Mean loss\t0.7606269663104264\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "DecisionTreeClassifier\n",
      "\n",
      "DecisionTreeClassifier()\n",
      "\n",
      "Fold # 1\n",
      "\tLog loss: 15.599104008376285\n",
      "\tAccuracy: 0.5483596804351522\n",
      "\tMCC: 0.20361499231430125\n",
      "................................................................................\n",
      "\n",
      "Fold # 2\n",
      "\tLog loss: 15.710651985854325\n",
      "\tAccuracy: 0.5451300356960734\n",
      "\tMCC: 0.1957524391460013\n",
      "................................................................................\n",
      "\n",
      "Fold # 3\n",
      "\tLog loss: 15.50780490631743\n",
      "\tAccuracy: 0.551003060183611\n",
      "\tMCC: 0.20035747395710538\n",
      "................................................................................\n",
      "\n",
      "Fold # 4\n",
      "\tLog loss: 15.507804906317427\n",
      "\tAccuracy: 0.551003060183611\n",
      "\tMCC: 0.20487296242730882\n",
      "................................................................................\n",
      "\n",
      "Fold # 5\n",
      "\tLog loss: 15.707451012646393\n",
      "\tAccuracy: 0.5452227133628018\n",
      "\tMCC: 0.19982149300116128\n",
      "................................................................................\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Min loss \t15.507804906317427\n",
      "Max loss \t15.710651985854325\n",
      "Mean loss\t15.60656336390237\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "RandomForestClassifier\n",
      "\n",
      "RandomForestClassifier(n_jobs=-1)\n",
      "\n",
      "Fold # 1\n",
      "\tLog loss: 0.8536405938277337\n",
      "\tAccuracy: 0.6229814720380759\n",
      "\tMCC: 0.26265906061692756\n",
      "................................................................................\n",
      "\n",
      "Fold # 2\n",
      "\tLog loss: 0.8357613472492598\n",
      "\tAccuracy: 0.6326704062553119\n",
      "\tMCC: 0.2950489087192845\n",
      "................................................................................\n",
      "\n",
      "Fold # 3\n",
      "\tLog loss: 0.8429735032739045\n",
      "\tAccuracy: 0.6280176810608636\n",
      "\tMCC: 0.2818856278389029\n",
      "................................................................................\n",
      "\n",
      "Fold # 4\n",
      "\tLog loss: 0.8388282829272926\n",
      "\tAccuracy: 0.6188371302278136\n",
      "\tMCC: 0.25216480636055183\n",
      "................................................................................\n",
      "\n",
      "Fold # 5\n",
      "\tLog loss: 0.8332273981662356\n",
      "\tAccuracy: 0.6382182930975858\n",
      "\tMCC: 0.3110221561267533\n",
      "................................................................................\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Min loss \t0.8332273981662356\n",
      "Max loss \t0.8536405938277337\n",
      "Mean loss\t0.8408862250888852\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "XGBClassifier\n",
      "\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=100, n_jobs=24,\n",
      "              num_parallel_tree=1, objective='multi:softprob', predictor='auto',\n",
      "              random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=None,\n",
      "              subsample=1, tree_method='exact', validate_parameters=1,\n",
      "              verbosity=None)\n",
      "\n",
      "Fold # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:16:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\tLog loss: 0.7581591598863536\n",
      "\tAccuracy: 0.6651368349481557\n",
      "\tMCC: 0.3687684128938171\n",
      "................................................................................\n",
      "\n",
      "Fold # 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:17:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\tLog loss: 0.7547931606412256\n",
      "\tAccuracy: 0.669386367499575\n",
      "\tMCC: 0.3777110804715972\n",
      "................................................................................\n",
      "\n",
      "Fold # 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:17:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\tLog loss: 0.7608451558775289\n",
      "\tAccuracy: 0.6581094865691941\n",
      "\tMCC: 0.35293863264965264\n",
      "................................................................................\n",
      "\n",
      "Fold # 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:18:08] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\tLog loss: 0.7571708789185216\n",
      "\tAccuracy: 0.6591295477728664\n",
      "\tMCC: 0.3566557567455144\n",
      "................................................................................\n",
      "\n",
      "Fold # 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:18:31] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\tLog loss: 0.7376107596580113\n",
      "\tAccuracy: 0.6739204352261136\n",
      "\tMCC: 0.3877212332047436\n",
      "................................................................................\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Min loss \t0.7376107596580113\n",
      "Max loss \t0.7608451558775289\n",
      "Mean loss\t0.7537158229963282\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "LGBMClassifier\n",
      "\n",
      "LGBMClassifier()\n",
      "\n",
      "Fold # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/lightgbm/basic.py:859: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.7498311856040399\n",
      "\tAccuracy: 0.670916199218086\n",
      "\tMCC: 0.3824801292539629\n",
      "................................................................................\n",
      "\n",
      "Fold # 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/lightgbm/basic.py:859: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.7438533488315806\n",
      "\tAccuracy: 0.6734659187489376\n",
      "\tMCC: 0.388042246562318\n",
      "................................................................................\n",
      "\n",
      "Fold # 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/lightgbm/basic.py:859: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.754271322949002\n",
      "\tAccuracy: 0.6688201292077525\n",
      "\tMCC: 0.3773596999285909\n",
      "................................................................................\n",
      "\n",
      "Fold # 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/lightgbm/basic.py:859: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.7491561124740119\n",
      "\tAccuracy: 0.666439986399184\n",
      "\tMCC: 0.3727049577248767\n",
      "................................................................................\n",
      "\n",
      "Fold # 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/lightgbm/basic.py:859: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.7260123470604949\n",
      "\tAccuracy: 0.6825909554573274\n",
      "\tMCC: 0.407233405989076\n",
      "................................................................................\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Min loss \t0.7260123470604949\n",
      "Max loss \t0.754271322949002\n",
      "Mean loss\t0.7446248633838259\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "ExtraTreesClassifier\n",
      "\n",
      "ExtraTreesClassifier(n_jobs=-1)\n",
      "\n",
      "Fold # 1\n",
      "\tLog loss: 0.8698432255259411\n",
      "\tAccuracy: 0.6156722760496346\n",
      "\tMCC: 0.24250293860968952\n",
      "................................................................................\n",
      "\n",
      "Fold # 2\n",
      "\tLog loss: 0.8486004091194415\n",
      "\tAccuracy: 0.6207717151113378\n",
      "\tMCC: 0.26161676105245085\n",
      "................................................................................\n",
      "\n",
      "Fold # 3\n",
      "\tLog loss: 0.8594565955300117\n",
      "\tAccuracy: 0.6162869772186331\n",
      "\tMCC: 0.2482468163806703\n",
      "................................................................................\n",
      "\n",
      "Fold # 4\n",
      "\tLog loss: 0.837833506679514\n",
      "\tAccuracy: 0.6140768446106767\n",
      "\tMCC: 0.24072649455931944\n",
      "................................................................................\n",
      "\n",
      "Fold # 5\n",
      "\tLog loss: 0.8370544091404911\n",
      "\tAccuracy: 0.629547772866372\n",
      "\tMCC: 0.2903125734619145\n",
      "................................................................................\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Min loss \t0.8370544091404911\n",
      "Max loss \t0.8698432255259411\n",
      "Mean loss\t0.85055762919908\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pos_vectorizer = TfidfVectorizer(ngram_range=(1,4))\n",
    "metrics_dict_5 = {}\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "text_vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "pos_vectorizer = TfidfVectorizer(ngram_range=(1,4))\n",
    "standard_scaler = StandardScaler()\n",
    "\n",
    "\n",
    "for model in models:\n",
    "    model_name = type(model).__name__\n",
    "    \n",
    "    print()\n",
    "    print(80*'*')\n",
    "    print(80*'*')\n",
    "\n",
    "    print(f\"{model_name}\")\n",
    "    print()\n",
    "    print(model.__str__())\n",
    "    \n",
    "    stratified_k = StratifiedKFold(n_splits = 5, shuffle=True, random_state = RANDOM_SEED)\n",
    "    losses = []\n",
    "\n",
    "    total_mcc = 0\n",
    "    total_acc = 0\n",
    "    for idx, (train_idxs, valid_idxs) in enumerate(stratified_k.split(X_train, Y_train)):\n",
    "        print(f\"\\nFold # {idx + 1}\")\n",
    "\n",
    "        x_train, y_train = X_train.iloc[train_idxs], Y_train.iloc[train_idxs]\n",
    "        x_valid, y_valid = X_train.iloc[valid_idxs], Y_train.iloc[valid_idxs]\n",
    "\n",
    "        y_train = y_train.to_numpy()\n",
    "        y_valid = y_valid.to_numpy()\n",
    "\n",
    "        text_vectorizer.fit(x_train.essay_text.drop_duplicates())\n",
    "\n",
    "        vectorized_discourse_text_train = text_vectorizer.transform(x_train.discourse_text)\n",
    "        vectorized_discourse_text_valid = text_vectorizer.transform(x_valid.discourse_text)\n",
    "\n",
    "        ohe_discourse_type_train = ohe.fit_transform(x_train.discourse_type.values.reshape(-1, 1))\n",
    "        ohe_discourse_type_test = ohe.transform(x_valid.discourse_type.values.reshape(-1, 1))\n",
    "\n",
    "        incorrect_count_train = standard_scaler.fit_transform(x_train['incorrect_words_len'].to_numpy().reshape(-1, 1))\n",
    "        incorrect_count_valid = standard_scaler.transform(x_valid['incorrect_words_len'].to_numpy().reshape(-1, 1))\n",
    "\n",
    "        rp_start_train = x_train['rp_start'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "        rp_end_train = x_train['rp_end'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "        wps_train = x_train['words_per_sentences'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "        sps_train = x_train['syll_per_sentences'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "        spw_train = x_train['syll_per_words'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "        cqt_train = x_train['contains_question'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "\n",
    "        rp_start_valid = x_valid['rp_start'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "        rp_end_valid = x_valid['rp_end'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "        wps_valid = x_valid['words_per_sentences'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "        sps_valid = x_valid['syll_per_sentences'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "        spw_valid = x_valid['syll_per_words'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "        cqt_valid = x_valid['contains_question'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "\n",
    "        pos_features_train = pos_vectorizer.fit_transform(x_train.discourse_pos.map(' '.join))\n",
    "        pos_features_valid = pos_vectorizer.transform(x_valid.discourse_pos.map(' '.join))\n",
    "\n",
    "        x_train = sparse.hstack([vectorized_discourse_text_train, ohe_discourse_type_train, pos_features_train, incorrect_count_train, rp_start_train, rp_end_train, wps_train, sps_train, spw_train,cqt_train])\n",
    "        x_valid = sparse.hstack([vectorized_discourse_text_valid, ohe_discourse_type_test, pos_features_valid, incorrect_count_valid,  rp_start_valid, rp_end_valid, wps_valid, sps_valid, spw_valid, cqt_valid])\n",
    "        \n",
    "\n",
    "        model.fit(x_train, y_train)\n",
    "\n",
    "        prob_predictions = model.predict_proba(x_valid)\n",
    "        actual_pred = model.predict(x_valid)\n",
    "\n",
    "        lloss = log_loss(y_valid, prob_predictions)\n",
    "        losses.append(lloss)\n",
    "\n",
    "        acc = accuracy_score(y_valid, actual_pred)\n",
    "        mcc = matthews_corrcoef(y_valid, actual_pred)\n",
    "\n",
    "        print(f\"\\tLog loss: {lloss}\")\n",
    "        print(f\"\\tAccuracy: {acc}\")\n",
    "        print(f\"\\tMCC: {mcc}\")\n",
    "        \n",
    "        total_mcc += mcc\n",
    "        total_acc += acc\n",
    "        \n",
    "        print(80*'.')\n",
    "\n",
    "    mean_mcc = total_mcc / 5\n",
    "    mean_acc = total_acc / 5\n",
    "\n",
    "    metrics_dict_5[model_name] = {\n",
    "        'min_loss': min(losses),\n",
    "        'max_loss': max(losses),\n",
    "        'mean_loss': np.mean(losses),\n",
    "        'mean_acc': mean_acc, \n",
    "        'mean_mcc': mean_mcc,\n",
    "\n",
    "    } \n",
    "    \n",
    "    print()\n",
    "    print(80*'*')\n",
    "\n",
    "    print()\n",
    "\n",
    "    print(f\"Min loss \\t{min(losses)}\")\n",
    "    print(f\"Max loss \\t{max(losses)}\")\n",
    "    print(f\"Mean loss\\t{np.mean(losses)}\")\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "HistGradientBoostingClassifier\n",
      "\n",
      "HistGradientBoostingClassifier()\n",
      "\n",
      "Fold # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.7546528345401939\n",
      "\tAccuracy: 0.6612272650008499\n",
      "\tMCC: 0.36306092393405237\n",
      "................................................................................\n",
      "\n",
      "Fold # 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.7447359591732509\n",
      "\tAccuracy: 0.6698963114057453\n",
      "\tMCC: 0.38391821296423895\n",
      "................................................................................\n",
      "\n",
      "Fold # 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.7534947934089871\n",
      "\tAccuracy: 0.6604896293777627\n",
      "\tMCC: 0.3628508770659367\n",
      "................................................................................\n",
      "\n",
      "Fold # 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.748426349226227\n",
      "\tAccuracy: 0.6678000680040802\n",
      "\tMCC: 0.37655232834272073\n",
      "................................................................................\n",
      "\n",
      "Fold # 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLog loss: 0.7238574134944064\n",
      "\tAccuracy: 0.6793607616456987\n",
      "\tMCC: 0.4017354443621247\n",
      "................................................................................\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Min loss \t0.7238574134944064\n",
      "Max loss \t0.7546528345401939\n",
      "Mean loss\t0.7450334699686131\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/home/hdd_n/mambaforge/envs/exp/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pos_vectorizer = TfidfVectorizer(ngram_range=(1,4))\n",
    "metrics_dict_6 = {}\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "text_vectorizer = TfidfVectorizer()\n",
    "pos_vectorizer = TfidfVectorizer(ngram_range=(1,4))\n",
    "standard_scaler = StandardScaler()\n",
    "\n",
    "\n",
    "for model in models_2:\n",
    "    model_name = type(model).__name__\n",
    "    \n",
    "    print()\n",
    "    print(80*'*')\n",
    "    print(80*'*')\n",
    "\n",
    "    print(f\"{model_name}\")\n",
    "    print()\n",
    "    print(model.__str__())\n",
    "    \n",
    "    stratified_k = StratifiedKFold(n_splits = 5, shuffle=True, random_state = RANDOM_SEED)\n",
    "    losses = []\n",
    "\n",
    "    total_mcc = 0\n",
    "    total_acc = 0\n",
    "    for idx, (train_idxs, valid_idxs) in enumerate(stratified_k.split(X_train, Y_train)):\n",
    "        print(f\"\\nFold # {idx + 1}\")\n",
    "\n",
    "        x_train, y_train = X_train.iloc[train_idxs], Y_train.iloc[train_idxs]\n",
    "        x_valid, y_valid = X_train.iloc[valid_idxs], Y_train.iloc[valid_idxs]\n",
    "\n",
    "        y_train = y_train.to_numpy()\n",
    "        y_valid = y_valid.to_numpy()\n",
    "\n",
    "        text_vectorizer.fit(x_train.essay_text.drop_duplicates())\n",
    "\n",
    "        vectorized_discourse_text_train = text_vectorizer.transform(x_train.discourse_text)\n",
    "        vectorized_discourse_text_valid = text_vectorizer.transform(x_valid.discourse_text)\n",
    "\n",
    "        ohe_discourse_type_train = ohe.fit_transform(x_train.discourse_type.values.reshape(-1, 1))\n",
    "        ohe_discourse_type_test = ohe.transform(x_valid.discourse_type.values.reshape(-1, 1))\n",
    "\n",
    "        incorrect_count_train = standard_scaler.fit_transform(x_train['incorrect_words_len'].to_numpy().reshape(-1, 1))\n",
    "        incorrect_count_valid = standard_scaler.transform(x_valid['incorrect_words_len'].to_numpy().reshape(-1, 1))\n",
    "\n",
    "        rp_start_train = x_train['rp_start'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "        rp_end_train = x_train['rp_end'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "        wps_train = x_train['words_per_sentences'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "        sps_train = x_train['syll_per_sentences'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "        spw_train = x_train['syll_per_words'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "        cqt_train = x_train['contains_question'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "\n",
    "        rp_start_valid = x_valid['rp_start'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "        rp_end_valid = x_valid['rp_end'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "        wps_valid = x_valid['words_per_sentences'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "        sps_valid = x_valid['syll_per_sentences'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "        spw_valid = x_valid['syll_per_words'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "        cqt_valid = x_valid['contains_question'].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "\n",
    "        pos_features_train = pos_vectorizer.fit_transform(x_train.discourse_pos.map(' '.join))\n",
    "        pos_features_valid = pos_vectorizer.transform(x_valid.discourse_pos.map(' '.join))\n",
    "\n",
    "        x_train = sparse.hstack([vectorized_discourse_text_train, ohe_discourse_type_train, pos_features_train, incorrect_count_train, rp_start_train, rp_end_train, wps_train, sps_train, spw_train,cqt_train])\n",
    "        x_valid = sparse.hstack([vectorized_discourse_text_valid, ohe_discourse_type_test, pos_features_valid, incorrect_count_valid,  rp_start_valid, rp_end_valid, wps_valid, sps_valid, spw_valid, cqt_valid])\n",
    "        \n",
    "        x_train = x_train.todense()\n",
    "        x_valid = x_valid.todense()\n",
    "\n",
    "        model.fit(x_train, y_train)\n",
    "\n",
    "        prob_predictions = model.predict_proba(x_valid)\n",
    "        actual_pred = model.predict(x_valid)\n",
    "\n",
    "        lloss = log_loss(y_valid, prob_predictions)\n",
    "        losses.append(lloss)\n",
    "\n",
    "        acc = accuracy_score(y_valid, actual_pred)\n",
    "        mcc = matthews_corrcoef(y_valid, actual_pred)\n",
    "\n",
    "        print(f\"\\tLog loss: {lloss}\")\n",
    "        print(f\"\\tAccuracy: {acc}\")\n",
    "        print(f\"\\tMCC: {mcc}\")\n",
    "        \n",
    "        total_mcc += mcc\n",
    "        total_acc += acc\n",
    "        \n",
    "        print(80*'.')\n",
    "\n",
    "    mean_mcc = total_mcc / 5\n",
    "    mean_acc = total_acc / 5\n",
    "\n",
    "    metrics_dict_6[model_name] = {\n",
    "        'min_loss': min(losses),\n",
    "        'max_loss': max(losses),\n",
    "        'mean_loss': np.mean(losses),\n",
    "        'mean_acc': mean_acc, \n",
    "        'mean_mcc': mean_mcc,\n",
    "\n",
    "    } \n",
    "    \n",
    "    print()\n",
    "    print(80*'*')\n",
    "\n",
    "    print()\n",
    "\n",
    "    print(f\"Min loss \\t{min(losses)}\")\n",
    "    print(f\"Max loss \\t{max(losses)}\")\n",
    "    print(f\"Mean loss\\t{np.mean(losses)}\")\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_loss\n",
      "('LGBMClassifier', 'tfidf_(1,1)', 0.7428919807782683)\n",
      "('LGBMClassifier', 'text_shallow_pos', 0.7436512491866434)\n",
      "('LGBMClassifier', 'tfidf_(1,2)', 0.7446248633838259)\n",
      "('HistGradientBoostingClassifier', 'hgbm_(1,1)', 0.7450334699686131)\n",
      "('LGBMClassifier', 'text_only', 0.7520834554471512)\n",
      "('XGBClassifier', 'tfidf_(1,2)', 0.7537158229963282)\n",
      "('XGBClassifier', 'tfidf_(1,1)', 0.7549697213605379)\n",
      "('XGBClassifier', 'text_shallow_pos', 0.7551037099463949)\n",
      "('XGBClassifier', 'text_only', 0.7602450654668177)\n",
      "('LogisticRegression', 'tfidf_(1,2)', 0.7606269663104264)\n",
      "('LogisticRegression', 'tfidf_(1,1)', 0.7631966117322929)\n",
      "('LogisticRegression', 'text_shallow_pos', 0.7633119986983758)\n",
      "('LogisticRegression', 'text_only', 0.7653257899281086)\n",
      "('LGBMClassifier', 'text_shallow', 0.7892973424897693)\n",
      "('XGBClassifier', 'text_shallow', 0.7952926134977212)\n",
      "('ExtraTreesClassifier', 'tfidf_(1,1)', 0.8094006927746504)\n",
      "('ExtraTreesClassifier', 'text_shallow_pos', 0.811773215438004)\n",
      "('RandomForestClassifier', 'text_shallow_pos', 0.8130713664051697)\n",
      "('RandomForestClassifier', 'tfidf_(1,1)', 0.8133160039211091)\n",
      "('RandomForestClassifier', 'text_only', 0.8152966738617605)\n",
      "('ExtraTreesClassifier', 'text_only', 0.817497525323461)\n",
      "('RandomForestClassifier', 'tfidf_(1,2)', 0.8408862250888852)\n",
      "('ExtraTreesClassifier', 'tfidf_(1,2)', 0.85055762919908)\n",
      "('LogisticRegression', 'text_shallow', 0.9037462046202298)\n",
      "('RandomForestClassifier', 'text_shallow', 0.939467983941754)\n",
      "('ExtraTreesClassifier', 'text_shallow', 1.1384579648879214)\n",
      "('DecisionTreeClassifier', 'tfidf_(1,2)', 15.60656336390237)\n",
      "('DecisionTreeClassifier', 'tfidf_(1,1)', 15.845016838087474)\n",
      "('DecisionTreeClassifier', 'text_shallow_pos', 15.847322675085792)\n",
      "('DecisionTreeClassifier', 'text_only', 15.903644020468912)\n",
      "('DecisionTreeClassifier', 'text_shallow', 16.70346096032301)\n",
      "********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "dicts_of_interest = [metrics_dict, metrics_dict_2, metrics_dict_3, metrics_dict_4, metrics_dict_5, metrics_dict_6]\n",
    "dicts_name = ['text_only', 'text_shallow', 'text_shallow_pos', 'tfidf_(1,1)', 'tfidf_(1,2)', 'hgbm_(1,1)']\n",
    "metric_of_interest = 'mean_loss'\n",
    "\n",
    "collector = []\n",
    "for idx, dict_of_interest in enumerate(dicts_of_interest):\n",
    "    for model_name, model_metrics in dict_of_interest.items():\n",
    "        collector.append((model_name, f\"{dicts_name[idx]}\", model_metrics[metric_of_interest]))\n",
    "\n",
    "print(f'{metric_of_interest}')\n",
    "for i in sorted(collector, key = lambda item: item[2]):\n",
    "    print(i)\n",
    "print(80*'*')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_loss\n",
      "('LGBMClassifier', 'tfidf_(1,1)', 0.7225697317441382)\n",
      "('HistGradientBoostingClassifier', 'hgbm_(1,1)', 0.7238574134944064)\n",
      "('LGBMClassifier', 'text_shallow_pos', 0.7244428339683451)\n",
      "('LGBMClassifier', 'tfidf_(1,2)', 0.7260123470604949)\n",
      "('LGBMClassifier', 'text_only', 0.7331417992302278)\n",
      "('XGBClassifier', 'tfidf_(1,1)', 0.7370382852622601)\n",
      "('XGBClassifier', 'tfidf_(1,2)', 0.7376107596580113)\n",
      "('XGBClassifier', 'text_shallow_pos', 0.7377060311779143)\n",
      "('XGBClassifier', 'text_only', 0.7425925319916824)\n",
      "('LogisticRegression', 'tfidf_(1,2)', 0.7461715144799874)\n",
      "('LogisticRegression', 'text_shallow_pos', 0.7468061674669159)\n",
      "('LogisticRegression', 'tfidf_(1,1)', 0.7472652309513227)\n",
      "('LogisticRegression', 'text_only', 0.748670305385369)\n",
      "('XGBClassifier', 'text_shallow', 0.7635146597786169)\n",
      "('LGBMClassifier', 'text_shallow', 0.7649250616062437)\n",
      "('ExtraTreesClassifier', 'text_only', 0.7995769027439527)\n",
      "('ExtraTreesClassifier', 'tfidf_(1,1)', 0.8044192804091473)\n",
      "('ExtraTreesClassifier', 'text_shallow_pos', 0.8047947912394996)\n",
      "('RandomForestClassifier', 'text_only', 0.804797833813115)\n",
      "('RandomForestClassifier', 'tfidf_(1,1)', 0.8056989356267727)\n",
      "('RandomForestClassifier', 'text_shallow_pos', 0.8100954494527491)\n",
      "('RandomForestClassifier', 'tfidf_(1,2)', 0.8332273981662356)\n",
      "('ExtraTreesClassifier', 'tfidf_(1,2)', 0.8370544091404911)\n",
      "('LogisticRegression', 'text_shallow', 0.8981872979545485)\n",
      "('RandomForestClassifier', 'text_shallow', 0.9098643184522397)\n",
      "('ExtraTreesClassifier', 'text_shallow', 1.0830011040786778)\n",
      "('DecisionTreeClassifier', 'text_only', 15.46670129619088)\n",
      "('DecisionTreeClassifier', 'tfidf_(1,2)', 15.507804906317427)\n",
      "('DecisionTreeClassifier', 'tfidf_(1,1)', 15.713440799041408)\n",
      "('DecisionTreeClassifier', 'text_shallow_pos', 15.801402121507081)\n",
      "('DecisionTreeClassifier', 'text_shallow', 16.429700162012935)\n",
      "********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "metric_of_interest = 'min_loss'\n",
    "\n",
    "collector = []\n",
    "for idx, dict_of_interest in enumerate(dicts_of_interest):\n",
    "    for model_name, model_metrics in dict_of_interest.items():\n",
    "        collector.append((model_name, f\"{dicts_name[idx]}\", model_metrics[metric_of_interest]))\n",
    "\n",
    "print(f'{metric_of_interest}')\n",
    "for i in sorted(collector, key = lambda item: item[2]):\n",
    "    print(i)\n",
    "print(80*'*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_mcc\n",
      "('LGBMClassifier', 'tfidf_(1,2)', 0.3855640878917649)\n",
      "('LGBMClassifier', 'tfidf_(1,1)', 0.3829213460750664)\n",
      "('LGBMClassifier', 'text_shallow_pos', 0.38206734125884845)\n",
      "('HistGradientBoostingClassifier', 'hgbm_(1,1)', 0.37762355733381475)\n",
      "('LGBMClassifier', 'text_only', 0.37538179632155655)\n",
      "('XGBClassifier', 'tfidf_(1,1)', 0.37135121025011675)\n",
      "('LogisticRegression', 'tfidf_(1,2)', 0.3699148075427776)\n",
      "('XGBClassifier', 'tfidf_(1,2)', 0.36875902319306497)\n",
      "('XGBClassifier', 'text_shallow_pos', 0.36781170561141396)\n",
      "('XGBClassifier', 'text_only', 0.3665788499538949)\n",
      "('LogisticRegression', 'text_shallow_pos', 0.3632610571144278)\n",
      "('LogisticRegression', 'text_only', 0.36194384592723133)\n",
      "('LogisticRegression', 'tfidf_(1,1)', 0.361667616886424)\n",
      "('LGBMClassifier', 'text_shallow', 0.34165372829625285)\n",
      "('XGBClassifier', 'text_shallow', 0.3343283918893384)\n",
      "('RandomForestClassifier', 'tfidf_(1,1)', 0.31492069943886036)\n",
      "('ExtraTreesClassifier', 'tfidf_(1,1)', 0.30986778169577267)\n",
      "('RandomForestClassifier', 'text_shallow_pos', 0.30843647764084725)\n",
      "('ExtraTreesClassifier', 'text_shallow_pos', 0.3078666947929868)\n",
      "('RandomForestClassifier', 'text_only', 0.3004514657526284)\n",
      "('ExtraTreesClassifier', 'text_only', 0.2987367149908701)\n",
      "('RandomForestClassifier', 'tfidf_(1,2)', 0.2805561119324841)\n",
      "('RandomForestClassifier', 'text_shallow', 0.2686146993193471)\n",
      "('ExtraTreesClassifier', 'tfidf_(1,2)', 0.256681116812809)\n",
      "('ExtraTreesClassifier', 'text_shallow', 0.2488515415102257)\n",
      "('DecisionTreeClassifier', 'tfidf_(1,2)', 0.2008838721691756)\n",
      "('DecisionTreeClassifier', 'tfidf_(1,1)', 0.19372445025854865)\n",
      "('DecisionTreeClassifier', 'text_shallow_pos', 0.19228571901144123)\n",
      "('DecisionTreeClassifier', 'text_only', 0.1882648696545507)\n",
      "('DecisionTreeClassifier', 'text_shallow', 0.17126772107124202)\n",
      "('LogisticRegression', 'text_shallow', 0.1554030473598827)\n",
      "********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "metric_of_interest = 'mean_mcc'\n",
    "\n",
    "collector = []\n",
    "for idx, dict_of_interest in enumerate(dicts_of_interest):\n",
    "    for model_name, model_metrics in dict_of_interest.items():\n",
    "        collector.append((model_name, f\"{dicts_name[idx]}\", model_metrics[metric_of_interest]))\n",
    "\n",
    "print(f'{metric_of_interest}')\n",
    "for i in reversed(sorted(collector, key = lambda item: item[2])):\n",
    "    print(i)\n",
    "print(80*'*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_acc\n",
      "('LGBMClassifier', 'tfidf_(1,2)', 0.6724466378062576)\n",
      "('LGBMClassifier', 'tfidf_(1,1)', 0.6710187717501365)\n",
      "('LGBMClassifier', 'text_shallow_pos', 0.6706446741725463)\n",
      "('HistGradientBoostingClassifier', 'hgbm_(1,1)', 0.6677548070868273)\n",
      "('LGBMClassifier', 'text_only', 0.66751662519435)\n",
      "('XGBClassifier', 'tfidf_(1,1)', 0.6661567863373179)\n",
      "('LogisticRegression', 'tfidf_(1,2)', 0.6656808097930037)\n",
      "('XGBClassifier', 'tfidf_(1,2)', 0.665136534403181)\n",
      "('XGBClassifier', 'text_shallow_pos', 0.6644908597626514)\n",
      "('XGBClassifier', 'text_only', 0.6641846680102182)\n",
      "('LogisticRegression', 'text_shallow_pos', 0.6596969073286332)\n",
      "('LogisticRegression', 'text_only', 0.6592549328244413)\n",
      "('LogisticRegression', 'tfidf_(1,1)', 0.6588469776995051)\n",
      "('LGBMClassifier', 'text_shallow', 0.6535428675100075)\n",
      "('XGBClassifier', 'text_shallow', 0.6484769796709645)\n",
      "('RandomForestClassifier', 'tfidf_(1,1)', 0.6426968409197532)\n",
      "('RandomForestClassifier', 'text_shallow_pos', 0.6402828174449944)\n",
      "('ExtraTreesClassifier', 'tfidf_(1,1)', 0.6398748738794802)\n",
      "('ExtraTreesClassifier', 'text_shallow_pos', 0.6391608252571986)\n",
      "('RandomForestClassifier', 'text_only', 0.6365768782774935)\n",
      "('ExtraTreesClassifier', 'text_only', 0.6357609680276208)\n",
      "('RandomForestClassifier', 'tfidf_(1,2)', 0.6281449965359301)\n",
      "('ExtraTreesClassifier', 'tfidf_(1,2)', 0.6192711171713309)\n",
      "('RandomForestClassifier', 'text_shallow', 0.6141714584806076)\n",
      "('ExtraTreesClassifier', 'text_shallow', 0.5983274787750226)\n",
      "('LogisticRegression', 'text_shallow', 0.5886372730213549)\n",
      "('DecisionTreeClassifier', 'tfidf_(1,2)', 0.5481437099722498)\n",
      "('DecisionTreeClassifier', 'tfidf_(1,1)', 0.5411738177008623)\n",
      "('DecisionTreeClassifier', 'text_shallow_pos', 0.5411396942868076)\n",
      "('DecisionTreeClassifier', 'text_only', 0.5395423439837803)\n",
      "('DecisionTreeClassifier', 'text_shallow', 0.5162860120068873)\n",
      "********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "metric_of_interest = 'mean_acc'\n",
    "\n",
    "collector = []\n",
    "for idx, dict_of_interest in enumerate(dicts_of_interest):\n",
    "    for model_name, model_metrics in dict_of_interest.items():\n",
    "        collector.append((model_name, f\"{dicts_name[idx]}\", model_metrics[metric_of_interest]))\n",
    "\n",
    "print(f'{metric_of_interest}')\n",
    "for i in reversed(sorted(collector, key = lambda item: item[2])):\n",
    "    print(i)\n",
    "print(80*'*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 ('exp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b7d9b83467eda07c7dcec41582b0623e164b4685431fddd627aa809781990b03"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
